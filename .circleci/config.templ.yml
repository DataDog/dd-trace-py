version: 2.1

default_resource_class: &default_resource_class medium
ubuntu_base_image: &ubuntu_base_img ubuntu-2004:2023.04.2
cimg_base_image: &cimg_base_image cimg/base:2022.08
python310_image: &python310_image cimg/python:3.10.12
ddtrace_dev_image: &ddtrace_dev_image ghcr.io/datadog/dd-trace-py/testrunner@sha256:4c8afd048321e702f3605b4ae4d206fcd00e74bac708089cfe7f9c24383dc53b
redis_image: &redis_image redis:4.0-alpine@sha256:3e99741f293147ff406657dda7644c2b88564b80a498cd00da8f905743449c9f
memcached_image: &memcached_image memcached:1.5-alpine@sha256:48cb7207e3d34871893fa1628f3a4984375153e9942facf82e25935b0a633c8a
cassandra_image: &cassandra_image cassandra:3.11.7@sha256:495e5752526f7e75d3ad85b6a6bbf3b79714321b17a44255a216c341e3baae11
consul_image: &consul_image consul:1.6.0@sha256:daa6203532fc30d81bf6c5593f79a2c7c23f08e8fde82f1e4bd8069b48b57596
moto_image: &moto_image datadog/docker-library:moto_1_0_1@sha256:58c15f03141073629f4ff2a78910b812205324579c76f8bcac87e8e89af2e673
mysql_image: &mysql_image mysql:5.7@sha256:03b6dcedf5a2754da00e119e2cc6094ed3c884ad36b67bb25fe67be4b4f9bdb1
postgres_image: &postgres_image postgres:12-alpine@sha256:c6704f41eb84be53d5977cb821bf0e5e876064b55eafef1e260c2574de40ad9a
mongo_image: &mongo_image mongo:3.6@sha256:19c11a8f1064fd2bb713ef1270f79a742a184cd57d9bb922efdd2a8eca514af8
httpbin_image: &httpbin_image kennethreitz/httpbin@sha256:2c7abc4803080c22928265744410173b6fea3b898872c01c5fd0f0f9df4a59fb
vertica_image: &vertica_image vertica/vertica-ce:latest
rabbitmq_image: &rabbitmq_image rabbitmq:3.7-alpine
testagent_image: &testagent_image ghcr.io/datadog/dd-apm-test-agent/ddapm-test-agent:v1.17.0

parameters:
  coverage:
    type: boolean
    default: false
  riot_run_latest:
    type: boolean
    default: false

orbs:
  win: circleci/windows@5.0

machine_executor: &machine_executor
  machine:
    image: *ubuntu_base_img
  environment:
    - BOTO_CONFIG: /dev/null
    # https://support.circleci.com/hc/en-us/articles/360045268074-Build-Fails-with-Too-long-with-no-output-exceeded-10m0s-context-deadline-exceeded-
    - PYTHONUNBUFFERED: 1
    - CMAKE_BUILD_PARALLEL_LEVEL: 12
  steps:
    - run: sudo apt-get update && sudo DEBIAN_FRONTEND=noninteractive apt-get install -o Dpkg::Options::="--force-confdef" -o Dpkg::Options::="--force-confold" --force-yes -y --no-install-recommends openssh-client git
    - &pyenv-set-global
      run:
        name: Set global pyenv
        command: |
          pyenv global 3.9.4

contrib_job_large: &contrib_job_large
  executor: ddtrace_dev
  parallelism: 10

contrib_job: &contrib_job
  executor: ddtrace_dev
  parallelism: 2

contrib_job_small: &contrib_job_small
  executor: ddtrace_dev_small
  parallelism: 1

commands:
  save_coverage:
    description: "Save coverage.py results to workspace"
    steps:
      - run: |
          set -ex
          mkdir coverage
          touch ./coverage/empty.$CIRCLE_BUILD_NUM-$CIRCLE_JOB-$CIRCLE_NODE_INDEX.coverage
          if [ -f .coverage ];
          then
            cp .coverage ./coverage/$CIRCLE_BUILD_NUM-$CIRCLE_JOB-$CIRCLE_NODE_INDEX.coverage
          fi
      - persist_to_workspace:
          root: coverage
          paths:
            - "*.coverage"
      - store_artifacts:
          path: coverage

  setup_riot:
    description: "Install riot"
    steps:
      # Make sure we install and run riot on Python 3
      - run: pip3 install riot==0.20.0

  setup_rust:
    description: "Install rust toolchain"
    steps:
      -  run: curl https://sh.rustup.rs -sSf | sh -s -- --default-toolchain stable -y
      -  run: echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> "$BASH_ENV"

  setup_sccache:
    description: "Install and configure sccache"
    steps:
      - run:
          name: "Install sccache"
          command: |
              curl -L https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz | tar zx --strip-components=1 -C /usr/local/bin/ sccache-v0.8.1-x86_64-unknown-linux-musl/sccache
              echo "export SCCACHE_DIR=/root/.cache/sccache" >> "$BASH_ENV"
              echo "export SCCACHE_CACHE_SIZE=1G" >> "$BASH_ENV"
              echo "export SCCACHE_DIRECT=true" >> "$BASH_ENV"
              echo "export DD_USE_SCCACHE=1" >> "$BASH_ENV"
              /usr/local/bin/sccache --version
      - restore_cache:
          name: Restore sccache cache
          key: v0-sccache-cache-{{ arch }}-{{ .Environment.CIRCLE_JOB }}-{{ .Environment.CIRCLE_NODE_INDEX }}

  save_sccache:
   description: "Save the sccache cache"
   steps:
     - run:
         name: "sccache stats"
         command: /usr/local/bin/sccache --show-stats
     - save_cache:
         name: Save sccache cache
         key: v0-sccache-cache-{{ arch }}-{{ .Environment.CIRCLE_JOB }}-{{ .Environment.CIRCLE_NODE_INDEX }}-{{ epoch }}
         paths:
           - /root/.cache/sccache/

  setup_hatch:
    description: "Install hatch"
    steps:
      - run:
          name: "Install hatch"
          command: |
            curl -L https://github.com/pypa/hatch/releases/download/hatch-v1.12.0/hatch-x86_64-unknown-linux-gnu.tar.gz | tar zx
            sudo install -t /usr/local/bin hatch
            hatch -q

  start_docker_services:
    description: "Start Docker services"
    parameters:
      env:
        type: string
        default: ""
      services:
        type: string
        default: ""
    steps:
      # Retry pulls in case they fail
      - run: for i in {1..3}; do docker-compose pull -q << parameters.services >> && break || sleep 3; done
      - run: << parameters.env >> docker-compose up -d << parameters.services >>
      - run:
          command: docker-compose logs -f
          background: true

  run_test:
    description: "Run tests matching a pattern"
    parameters:
      pattern:
        type: string
        default: ""
      wait:
        type: string
        default: ""
      snapshot:
        type: boolean
        default: false
      docker_services:
        type: string
        default: ""
      store_coverage:
        type: boolean
        default: true
      trace_agent_url:
        type: string
        default: "http://localhost:9126"
      run_agent_checks:
        type: boolean
        default: true
    steps:
      - checkout
      - attach_workspace:
          at: .
      - restore_cache:
          keys:
            - lastsuccess-{{ .Environment.CIRCLE_BRANCH }}-<<parameters.pattern>>-{{ .Environment.CIRCLE_NODE_INDEX }}
      - when:
          condition:
              << parameters.snapshot >>
          steps:
            - setup_riot
            - start_docker_services:
                env: SNAPSHOT_CI=1
                services: testagent << parameters.docker_services >>
            - run:
                environment:
                  DD_TRACE_AGENT_URL: << parameters.trace_agent_url >>
                  RIOT_RUN_RECOMPILE_REQS: "<< pipeline.parameters.riot_run_latest >>"
                no_output_timeout: 5m
                command: |
                  ulimit -c unlimited
                  ./scripts/run-test-suite '<<parameters.pattern>>' <<pipeline.parameters.coverage>> 1
            - run:
                command: |
                  mkdir -p /tmp/core_dumps
                  cp core.* /tmp/core_dumps || true
                  ./scripts/bt
                when: on_fail
            - store_artifacts:
                name: "Store core dumps"
                path: /tmp/core_dumps
      - unless:
          condition:
              << parameters.snapshot >>
          steps:
            - setup_riot
            - when:
                condition:
                  << parameters.wait >>
                steps:
                  - run:
                      name: "Waiting for << parameters.wait >>"
                      command: riot -P -v run -s 'wait' << parameters.wait >>
            - when:
                condition:
                  << parameters.trace_agent_url >> != ""
                steps:
                  - run:
                      command: |
                        echo 'export DD_TRACE_AGENT_URL=<< parameters.trace_agent_url >>' >> "$BASH_ENV"
                        source "$BASH_ENV"
            - run:
                environment:
                  RIOT_RUN_RECOMPILE_REQS: "<< pipeline.parameters.riot_run_latest >>"
                no_output_timeout: 5m
                command: |
                  ulimit -c unlimited
                  ./scripts/run-test-suite '<<parameters.pattern>>' <<pipeline.parameters.coverage>>
            - run:
                command: |
                  mkdir -p /tmp/core_dumps
                  cp core.* /tmp/core_dumps || true
                  ./scripts/bt
                when: on_fail
            - store_artifacts:
                name: "Store core dumps"
                path: /tmp/core_dumps
      - when:
          condition:
            << parameters.store_coverage >>
          steps:
            - save_coverage
      - store_test_results:
          path: test-results
      - store_artifacts:
          path: test-results
      - run:
          name: "Store Test Agent Supported Integrations Data"
          command: |
            if [[ -z "$(curl -s << parameters.trace_agent_url >>/test/integrations/tested_versions)" ]]; then
              # No integrations were tested. Not saving any artifacts
              echo "Response body is empty. Skipping saving integration artifacts."
            else
              # make temporary files to save response data to
              response=$(mktemp) && headers=$(mktemp)
              # create artifacts directory if it doesn't exist
              [ -d "./artifacts" ] || mkdir -p "./artifacts"
              # get tested integrations
              curl -o "$response" -D "$headers" << parameters.trace_agent_url >>/test/integrations/tested_versions
              # get filename representing the name of the tested integration from headers
              filename=$(awk -F': ' '/file-name/{print $2}' "$headers" | tr -d '\r\n')
              # copy data to final file and remove temp files
              mv "$response" "artifacts/${filename}_supported_versions.csv"
              rm "$headers"
            fi
      - store_artifacts:
          path: artifacts
          destination: supported-integrations
      - save_cache:
          key: lastsuccess-{{ .Environment.CIRCLE_BRANCH }}-<<parameters.pattern>>-{{ .Environment.CIRCLE_NODE_INDEX }}-{{ epoch }}
          paths:
            - ./latest-success-commit
      - when:
          condition:
              << parameters.run_agent_checks >>
          steps:
            - run:
                name: Get APM Test Agent Trace Check Results
                command: bash ./scripts/get-test-agent-results.sh

  run_hatch_env_test:
    description: "Run hatch env test"
    parameters:
      env:
        type: string
        default: ""
      snapshot:
        type: boolean
        default: false
      docker_services:
        type: string
        default: ""
      store_coverage:
        type: boolean
        default: true
      trace_agent_url:
        type: string
        default: "http://localhost:9126"
      run_agent_checks:
        type: boolean
        default: true
    steps:
      - checkout
      - attach_workspace:
          at: .
      - restore_cache:
          keys:
            - lastsuccess-{{ .Environment.CIRCLE_BRANCH }}-<<parameters.env>>-{{ .Environment.CIRCLE_NODE_INDEX }}
      - setup_hatch
      - when:
          condition:
              << parameters.snapshot >>
          steps:
            - start_docker_services:
                env: SNAPSHOT_CI=1
                services: testagent << parameters.docker_services >>
            - run:
                name: Run tests
                environment:
                  DD_TRACE_AGENT_URL: << parameters.trace_agent_url >>
                command: |
                  ulimit -c unlimited
                  ./scripts/run-test-suite-hatch '<<parameters.env>>' 1
            - run:
                command: |
                  mkdir -p /tmp/core_dumps
                  cp core.* /tmp/core_dumps || true
                  ./scripts/bt
                when: on_fail
            - store_artifacts:
                name: "Store core dumps"
                path: /tmp/core_dumps
      - unless:
          condition:
              << parameters.snapshot >>
          steps:
            - run:
                name: Run tests
                command: |
                  hatch env show --json | jq -r 'keys[] | select(. | contains("<< parameters.env >>"))' | sort | circleci tests split | xargs -n 1 -I {} hatch run {}:test
      - when:
          condition:
            << parameters.store_coverage >>
          steps:
            - save_coverage
      - store_test_results:
          path: test-results
      - store_artifacts:
          path: test-results
      - save_cache:
          key: lastsuccess-{{ .Environment.CIRCLE_BRANCH }}-<<parameters.env>>-{{ .Environment.CIRCLE_NODE_INDEX }}-{{ epoch }}
          paths:
            - ./latest-success-commit
      - run:
          name: "Store Test Agent Supported Integrations Data"
          command: |
            if [[ -z "$(curl -s << parameters.trace_agent_url >>/test/integrations/tested_versions)" ]]; then
              # No integrations were tested. Not saving any artifacts
              echo "Response body is empty. Skipping saving integration artifacts."
            else
              # make temporary files to save response data to
              response=$(mktemp) && headers=$(mktemp)
              # create artifacts directory if it doesn't exist
              [ -d "./artifacts" ] || mkdir -p "./artifacts"
              # get tested integrations
              curl -o "$response" -D "$headers" << parameters.trace_agent_url >>/test/integrations/tested_versions
              # get filename representing the name of the tested integration from headers
              filename=$(awk -F': ' '/file-name/{print $2}' "$headers" | tr -d '\r\n')
              # copy data to final file and remove temp files
              mv "$response" "artifacts/${filename}_supported_versions.csv"
              rm "$headers"
            fi
      - store_artifacts:
          path: artifacts
          destination: supported-integrations
      - when:
          condition:
              << parameters.run_agent_checks >>
          steps:
            - run:
                name: Get APM Test Agent Trace Check Results
                command: bash ./scripts/get-test-agent-results.sh

executors:
  cimg_base:
    docker:
      - image: *cimg_base_image
        environment:
          - CMAKE_BUILD_PARALLEL_LEVEL: 6
    resource_class: medium
  python310:
    docker:
      - image: *python310_image
        environment:
          - CMAKE_BUILD_PARALLEL_LEVEL: 12
    resource_class: large
  ddtrace_dev:
    docker:
      - image: *ddtrace_dev_image
        environment:
          - CMAKE_BUILD_PARALLEL_LEVEL: 6
    resource_class: *default_resource_class
  ddtrace_dev_small:
    docker:
      - image: *ddtrace_dev_image
        environment:
          - CMAKE_BUILD_PARALLEL_LEVEL: 4
    resource_class: small

# Common configuration blocks as YAML anchors
# See: https://circleci.com/blog/circleci-hacks-reuse-yaml-in-your-circleci-config-with-yaml/
httpbin_local: &httpbin_local
  image: *httpbin_image
  name: httpbin.org

mysql_server: &mysql_server
  image: *mysql_image
  environment:
    - MYSQL_ROOT_PASSWORD=admin
    - MYSQL_PASSWORD=test
    - MYSQL_USER=test
    - MYSQL_DATABASE=test

postgres_server: &postgres_server
  image: *postgres_image
  environment:
    - POSTGRES_PASSWORD=postgres
    - POSTGRES_USER=postgres
    - POSTGRES_DB=postgres

testagent: &testagent
  image: *testagent_image
  environment:
    - LOG_LEVEL=DEBUG
    - SNAPSHOT_DIR=/snapshots
    - PORT=9126
    - SNAPSHOT_CI=1
    - DD_POOL_TRACE_CHECK_FAILURES=true
    - DD_DISABLE_ERROR_RESPONSES=true
    - ENABLED_CHECKS=trace_content_length,trace_stall,meta_tracer_version_header,trace_count_header,trace_peer_service,trace_dd_service

jobs:
  pre_check:
    executor: python310
    steps:
      - checkout
      - setup_hatch
      - run:
          name: "Spelling"
          command: hatch run lint:spelling

  coverage_report:
    executor: python310
    steps:
      - checkout
      - attach_workspace:
          at: .
      - run: pip install -r ci/coverage/requirements.txt
      - run: rm -f empty.*.coverage
      # Combine all job coverage reports into one
      - run: ls -hal *.coverage 2>/dev/null && coverage combine *.coverage || true
      # Upload coverage report to Codecov
      # DEV: Do not use the bash uploader, it cannot be trusted
      - run: codecov
      # Generate and save xml report
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage xml --ignore-errors || true
      - store_artifacts:
          path: coverage.xml
      # Generate and save JSON report
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage json --ignore-errors || true
      - store_artifacts:
          path: coverage.json
      # Print ddtrace/ report to stdout
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage report --ignore-errors --omit='tests/*' || true
      # Print tests/ report to stdout
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage report --ignore-errors --omit='ddtrace/*' || true
      # Print diff-cover report to stdout (compares against origin/1.x)
      - run: diff-cover --compare-branch $(git rev-parse --abbrev-ref origin/HEAD) coverage.xml || true


  build_base_venvs:
    resource_class: large
    docker:
      - image: *ddtrace_dev_image
    parallelism: 6
    steps:
      - checkout
      - setup_riot
      - setup_sccache
      - run:
          name: "Generate base virtual environments."
          # DEV: riot list -i tracer lists all supported Python versions
          command: "riot list -i tracer | circleci tests split | xargs -I PY riot -P -v generate --python=PY"
          environment:
            CMAKE_BUILD_PARALLEL_LEVEL: 12
            PIP_VERBOSE: 1
      - save_sccache
      - persist_to_workspace:
          root: .
          paths:
            - "."

  appsec_iast_packages:
    <<: *machine_executor
    parallelism: 5
    steps:
      - when:
          condition:
            matches: { pattern: "main", value: << pipeline.git.branch >> }
          steps:
            - run_test:
                pattern: 'appsec_iast_packages'
                snapshot: true
      - run: echo "This test is skipped outside of main branch"

  appsec_integrations:
    <<: *machine_executor
    parallelism: 7
    steps:
      - run_test:
          pattern: 'appsec_integrations'
          snapshot: true
          run_agent_checks: false
          docker_services: "pygoat"

  aws_lambda:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'aws_lambda'
          snapshot: true

  datastreams:
    <<: *contrib_job_small
    steps:
      - run_test:
          pattern: "datastreams"

  ci_visibility:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: "ci_visibility"
          snapshot: true

  dd_coverage:
    <<: *machine_executor
    parallelism: 6
    steps:
      - run_hatch_env_test:
          env: 'dd_coverage'
          snapshot: true

  sourcecode:
    <<: *contrib_job_small
    steps:
      - run_test:
          pattern: "sourcecode"

  opentelemetry:
    parallelism: 4
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'opentelemetry'
          snapshot: true

  botocore:
    <<: *machine_executor
    parallelism: 6
    steps:
      - run_test:
          pattern: 'botocore'
          snapshot: true
          docker_services: "localstack"

  asyncpg:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'asyncpg'
          snapshot: true
          docker_services: 'postgres'

  aiohttp:
    <<: *machine_executor
    parallelism: 3
    steps:
      - run_test:
          pattern: 'aiohttp'  # includes aiohttp_jinja2
          snapshot: true
          docker_services: 'httpbin_local'

  cassandra:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
        environment:
          CASS_DRIVER_NO_EXTENSIONS: 1
      - image: *cassandra_image
        environment:
          - MAX_HEAP_SIZE=512M
          - HEAP_NEWSIZE=256M
      - *testagent
    steps:
      - run_test:
          wait: cassandra
          pattern: 'cassandra'

  celery:
    <<: *contrib_job_large
    docker:
      - image: *ddtrace_dev_image
      - image: *redis_image
      - image: *rabbitmq_image
      - image: *testagent_image
        environment:
        - LOG_LEVEL=DEBUG
        - SNAPSHOT_DIR=/snapshots
        - PORT=9126
        - SNAPSHOT_CI=1
        - DD_POOL_TRACE_CHECK_FAILURES=true
        - DD_DISABLE_ERROR_RESPONSES=true
        - ENABLED_CHECKS=trace_stall,meta_tracer_version_header,trace_content_length,trace_peer_service,trace_dd_service # disable flaky content length check
    steps:
      - run_test:
          pattern: 'celery'

  consul:
    <<: *contrib_job_small
    docker:
      - image: *ddtrace_dev_image
      - image: *consul_image
      - *testagent
    steps:
      - run_test:
          pattern: 'consul'

  django:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: 'django($|_celery)'
          snapshot: true
          docker_services: "memcached redis postgres"

  djangorestframework:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: 'djangorestframework'
          snapshot: true
          docker_services: "memcached redis"

  dramatiq:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: "dramatiq"
          snapshot: true
          docker_services: "redis"

  httplib:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "httplib"
          snapshot: true
          docker_services: 'httpbin_local'

  httpx:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: 'httpx'
          snapshot: true
          docker_services: 'httpbin_local'

  mariadb:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: 'mariadb$'
          snapshot: true
          docker_services: "mariadb"

  mysqlconnector:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
      - *testagent
    steps:
      - run_test:
          wait: mysql
          pattern: 'mysql$'

  mysqlpython:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
      - *testagent
    steps:
      - run_test:
          wait: mysql
          pattern: 'mysqldb$'

  pymysql:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
      - *testagent
    steps:
      - run_test:
          wait: mysql
          pattern: 'pymysql$'

  pylibmc:
    <<: *contrib_job_small
    docker:
      - image: *ddtrace_dev_image
      - image: *memcached_image
      - *testagent
    steps:
      - run_test:
          pattern: 'pylibmc'

  pytest:
    <<: *machine_executor
    parallelism: 10
    steps:
      - run_test:
          pattern: 'pytest'
          snapshot: true

  pymemcache:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *memcached_image
      - *testagent
    steps:
      - run_test:
          pattern: "pymemcache"

  mongoengine:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'mongoengine'
          snapshot: true
          docker_services: 'mongo'

  requests:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
      - *httpbin_local
    steps:
      - run_test:
          pattern: "requests"

  sqlalchemy:
    <<: *contrib_job_small
    docker:
      - image: *ddtrace_dev_image
      - *testagent
      - *postgres_server
      - *mysql_server
    steps:
      - run_test:
          wait: postgres mysql
          pattern: "sqlalchemy"

  psycopg:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          # We want psycopg and psycopg2 collected
          pattern: "psycopg"
          snapshot: true
          docker_services: "postgres"

  aiobotocore:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *moto_image
      - *testagent
    steps:
       - run_test:
          pattern: 'aiobotocore'

  aiomysql:
    <<: *machine_executor
    steps:
      - run_test:
          docker_services: 'mysql'
          wait: mysql
          pattern: 'aiomysql'
          snapshot: true

  aiopg:
    <<: *contrib_job_small
    parallelism: 4
    docker:
      - image: *ddtrace_dev_image
      - *postgres_server
      - *testagent
    steps:
      - run_test:
          wait: postgres
          pattern: 'aiopg'

  kombu:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *rabbitmq_image
      - *testagent
    steps:
      - run_test:
          wait: rabbitmq
          pattern: 'kombu'

  build_docs:
    # build documentation and store as an artifact
    executor: ddtrace_dev
    steps:
      - checkout
      - run:
          command: |
             hatch run docs:build
             mkdir -p /tmp/docs
             cp -r docs/_build/html/* /tmp/docs
      - store_artifacts:
          path: /tmp/docs

requires_pre_check: &requires_pre_check
  requires:
    - pre_check
requires_base_venvs: &requires_base_venvs
  requires:
    - pre_check
    - build_base_venvs
requires_tests: &requires_tests
  requires:


workflows:
  version: 2
  test: &workflow_test
    jobs:
      # Pre-checking before running all jobs
      - pre_check
