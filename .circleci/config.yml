version: 2.1

s3_dir_dev: &s3_dir_dev trace-dev
s3_dir_prod: &s3_dir_prod trace
s3_bucket: &s3_bucket pypi.datadoghq.com

resource_class: &resource_class medium
cimg_base_image: &cimg_base_image cimg/base:stable
python38-alpine_image: &python38-alpine_image python:3.8-alpine
python38_image: &python38_image circleci/python:3.8
python37_image: &python37_image circleci/python:3.7
python36_image: &python36_image circleci/python:3.6
python35_image: &python35_image circleci/python:3.5
python27_image: &python27_image circleci/python:2.7
ddtrace_dev_image: &ddtrace_dev_image datadog/dd-trace-py:latest
datadog_agent_image: &datadog_agent_image datadog/agent:latest
redis_image: &redis_image redis:4.0-alpine
rediscluster_image: &rediscluster_image grokzen/redis-cluster:6.2.0
memcached_image: &memcached_image memcached:1.5-alpine
cassandra_image: &cassandra_image cassandra:3.11.7
consul_image: &consul_image consul:1.6.0
moto_image: &moto_image palazzem/moto:1.0.1
elasticsearch_image: &elasticsearch_image elasticsearch:2.3
mysql_image: &mysql_image mysql:5.7
postgres_image: &postgres_image postgres:10.5-alpine
mongo_image: &mongo_image mongo:3.6
httpbin_image: &httpbin_image kennethreitz/httpbin@sha256:2c7abc4803080c22928265744410173b6fea3b898872c01c5fd0f0f9df4a59fb
vertica_image: &vertica_image sumitchawla/vertica:latest
rabbitmq_image: &rabbitmq_image rabbitmq:3.7-alpine

orbs:
  win: circleci/windows@2.2.0

machine_executor: &machine_executor
  machine:
    image: ubuntu-1604:201903-01
  environment:
    - BOTO_CONFIG: /dev/null
    # https://support.circleci.com/hc/en-us/articles/360045268074-Build-Fails-with-Too-long-with-no-output-exceeded-10m0s-context-deadline-exceeded-
    - PYTHONUNBUFFERED: 1
  steps:
    - &pyenv-set-global
      run:
        name: Set global pyenv
        command: |
          pyenv global 3.6.5

contrib_job: &contrib_job
  executor: ddtrace_dev
  parallelism: 4

commands:
  deploy_docs:
    description: "Deploy docs"
    parameters:
      s3_dir:
        type: string
      s3_bucket:
        type: string
        default: *s3_bucket
    steps:
      - build_docs
      - run: pip install awscli
      - run:
          name: Release docs
          command: aws s3 cp --recursive docs/_build/html/ "s3://<< parameters.s3_bucket >>/<< parameters.s3_dir >>/docs/"

  deploy_wheels:
    description: "Deploy wheels"
    parameters:
      s3_dir:
        type: string
        default: *s3_dir_dev
      s3_bucket:
        type: string
        default: *s3_bucket
    steps:
      - run: pip install awscli cython mkwheelhouse
      - run:
          name: Release wheels to dev site
          command: scripts/mkwheelhouse
          environment:
            S3_DIR: << parameters.s3_dir >>
            S3_BUCKET: << parameters.s3_bucket >>

  save_coverage:
    description: "Save coverage.py results to workspace"
    steps:
      - run: |
          set -ex
          mkdir coverage
          if [ -f .coverage ];
          then
            cp .coverage ./coverage/$CIRCLE_BUILD_NUM-$CIRCLE_JOB-$CIRCLE_NODE_INDEX.coverage
          fi
      - persist_to_workspace:
          root: coverage
          paths:
            - "*.coverage"
      - store_artifacts:
          path: coverage

  setup_tox:
    description: "Install tox"
    steps:
      - run: pip install -U tox

  setup_riot:
    description: "Install riot"
    steps:
      # Make sure we install and run riot on Python 3
      - run: pip3 install -U riot

  restore_tox_cache:
    description: "Restore .tox directory from previous runs for faster installs"
    steps:
      - restore_cache:
          # In the cache key:
          #   - .Environment.CIRCLE_JOB: We do separate tox environments by job name, so caching and restoring is
          #                              much faster.
          key: tox-cache-{{ .Environment.CIRCLE_JOB }}-{{ checksum "tox.ini" }}

  save_tox_cache:
    description: "Save .tox directory into cache for faster installs next time"
    steps:
      - save_cache:
          # In the cache key:
          #   - .Environment.CIRCLE_JOB: We do separate tox environments by job name, so caching and restoring is
          #                              much faster.
          key: tox-cache-{{ .Environment.CIRCLE_JOB }}-{{ checksum "tox.ini" }}
          paths:
            - ".tox"

  run_test:
    description: "Run tests matching a pattern"
    parameters:
      pattern:
        type: string
        default: ""
      wait:
        type: string
        default: ""
      snapshot:
        type: boolean
        default: false
      docker_services:
        type: string
        default: ""
      store_coverage:
        type: boolean
        default: true
    steps:
      - attach_workspace:
          at: .
      - checkout
      - when:
          condition:
              << parameters.snapshot >>
          steps:
            - run: SNAPSHOT_CI=1 docker-compose up -d testagent << parameters.docker_services >>
            - run:
                command: docker-compose logs -f
                background: true
            - run:
                environment:
                  DD_TRACE_AGENT_URL: http://localhost:9126
                command: "echo -p2.7,-p3.5,-p3.6,-p3.7,-p3.8,-p3.9 | tr ',' '\n' | circleci tests split | xargs ./scripts/ddtest riot -v run --pass-env -s '<< parameters.pattern >>'"
      - unless:
          condition:
              << parameters.snapshot >>
          steps:
            - when:
                condition:
                  << parameters.wait >>
                steps:
                  - setup_tox
                  - run:
                      name: "Waiting for << parameters.wait >>"
                      command: tox -e 'wait' << parameters.wait >>
            - setup_riot
            - run:
                command: "echo -p2.7,-p3.5,-p3.6,-p3.7,-p3.8,-p3.9 | tr ',' '\n' | circleci tests split | xargs riot -v run -s '<< parameters.pattern >>'"
      - when:
          condition:
            << parameters.store_coverage >>
          steps:
            - save_coverage
      - store_test_results:
          path: test-results
      - store_artifacts:
          path: test-results

  run_tox_scenario_with_testagent:
    description: Run scripts/run-tox-scenario with setup, caching persistence and the testagent
    parameters:
      pattern:
        type: string
      wait:
        type: string
        default: ""
    steps:
      - checkout
      - restore_tox_cache
      - when:
          condition:
            << parameters.wait >>
          steps:
            - run:
                name: "Waiting for << parameters.wait >>"
                command: tox -e 'wait' << parameters.wait >>
      - run: SNAPSHOT_CI=1 docker-compose up -d memcached redis testagent
      - run:
          command: docker-compose logs -f
          background: true
      - run:
          name: "Run scripts/run-tox-scenario"
          environment:
            DD_TRACE_AGENT_URL: http://localhost:9126
          command: ./scripts/ddtest scripts/run-tox-scenario '<< parameters.pattern >>'
      - save_tox_cache

  run_tox_scenario:
    description: "Run scripts/run-tox-scenario with setup, caching and persistence"
    parameters:
      pattern:
        type: string
      wait:
        type: string
        default: ""
      store_coverage:
        type: boolean
        default: true
    steps:
      - checkout
      - setup_tox
      - restore_tox_cache
      - when:
          condition:
            << parameters.wait >>
          steps:
            - run:
                name: "Waiting for << parameters.wait >>"
                command: tox -e 'wait' << parameters.wait >>
      - run:
          name: "Run scripts/run-tox-scenario"
          command: scripts/run-tox-scenario '<< parameters.pattern >>'
      - save_tox_cache
      - when:
          condition:
            << parameters.store_coverage >>
          steps:
            - save_coverage
      - store_test_results:
          path: test-results
      - store_artifacts:
          path: test-results

  test_build:
    description: "Build the package extensions and wheel to validate builds work"
    steps:
      - checkout
      - run:
          name: Run test build
          command: .circleci/scripts/test_build.sh

  build_docs:
    steps:
      - setup_tox
      - run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends libenchant-dev
      - run: tox -e docs

executors:
  cimg_base:
    docker:
      - image: *cimg_base_image
    resource_class: *resource_class
  python38-alpine:
    docker:
      - image: *python38-alpine_image
    resource_class: *resource_class
  python38:
    docker:
      - image: *python38_image
    resource_class: *resource_class
  python37:
    docker:
      - image: *python37_image
    resource_class: *resource_class
  python36:
    docker:
      - image: *python36_image
    resource_class: *resource_class
  python35:
    docker:
      - image: *python35_image
    resource_class: *resource_class
  python27:
    docker:
      - image: *python27_image
    resource_class: *resource_class
  ddtrace_dev:
    docker:
      - image: *ddtrace_dev_image
    resource_class: *resource_class

# Common configuration blocks as YAML anchors
# See: https://circleci.com/blog/circleci-hacks-reuse-yaml-in-your-circleci-config-with-yaml/
httpbin_local: &httpbin_local
  image: *httpbin_image
  name: httpbin.org

deploy_docs_filters: &deploy_docs_filters
  filters:
    tags:
      only: /(^docs$)|(^v[0-9]+(\.[0-9]+)*$)/
    branches:
      ignore: /.*/

datadog_agent: &datadog_agent
  image: *datadog_agent_image
  environment:
    DD_API_KEY: invalid_key_but_this_is_fine
    DD_APM_ENABLED: true
    DD_APM_RECEIVER_SOCKET: /tmp/ddagent/trace.sock
    DD_BIND_HOST: 0.0.0.0
    DD_LOG_LEVEL: DEBUG

mysql_server: &mysql_server
  image: *mysql_image
  environment:
    - MYSQL_ROOT_PASSWORD=admin
    - MYSQL_PASSWORD=test
    - MYSQL_USER=test
    - MYSQL_DATABASE=test

postgres_server: &postgres_server
  image: *postgres_image
  environment:
    - POSTGRES_PASSWORD=postgres
    - POSTGRES_USER=postgres
    - POSTGRES_DB=postgres

jobs:
  black:
    executor: python38
    steps:
      - checkout
      - setup_riot
      - run: riot run -s black --check .

  ccheck:
    executor: cimg_base
    steps:
      - checkout
      - run: sudo apt-get update
      - run: sudo apt-get install --yes clang-format gcc-10 python3 python3-setuptools cython3
      - run: scripts/cformat.sh
      - run: DD_COMPILE_DEBUG=1 CC=gcc-10 python3 setup.py build_ext

  flake8:
    executor: python38
    steps:
      - checkout
      - setup_riot
      - run: riot run -s flake8

  mypy:
    executor: python38
    steps:
      - checkout
      - setup_riot
      - run: riot run mypy

  coverage_report:
    executor: python38
    steps:
      - checkout
      - attach_workspace:
          at: .
      - run: pip install coverage diff_cover
      - run: ls -hal *.coverage
      # Combine all job coverage reports into one
      - run: coverage combine *.coverage
      # Upload coverage report to Codecov
      - run: bash <(curl -s https://codecov.io/bash)
      # Generate and save xml report
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage xml --ignore-errors
      - store_artifacts:
          path: coverage.xml
      # Generate and save JSON report
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage json --ignore-errors
      - store_artifacts:
          path: coverage.json
      # Generate and save HTML report
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage html --ignore-errors
      - store_artifacts:
          path: htmlcov
      # Print ddtrace/ report to stdout
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage report --ignore-errors --omit=tests/
      # Print tests/ report to stdout
      # DEV: "--ignore-errors" to skip over files that are missing
      - run: coverage report --ignore-errors --omit=ddtrace/
      # Print diff-cover report to stdout (compares against origin/master)
      - run: diff-cover coverage.xml

  build_base_venvs:
    executor: ddtrace_dev
    parallelism: 6
    steps:
      - checkout
      - setup_riot
      - run:
          name: "Generate base virtual environments."
          command: "echo '2.7,3.5,3.6,3.7,3.8,3.9' | tr ',' '\n' | circleci tests split | xargs -I PY riot -v generate --python=PY"
      - persist_to_workspace:
          root: .
          paths:
            - "."

  build-docker-ci-image:
    executor: cimg_base
    steps:
      - checkout
      - setup_remote_docker:
          docker_layer_caching: true
      - run: |
          docker build .

  test_build_alpine:
    executor: python38-alpine
    steps:
      - run: apk add git gcc musl-dev libffi-dev openssl-dev bash
      - test_build
  test_build_py38:
    executor: python38
    steps:
      - test_build
  test_build_py37:
    executor: python37
    steps:
      - test_build
  test_build_py36:
    executor: python36
    steps:
      - test_build
  test_build_py35:
    executor: python35
    steps:
      - test_build
  test_build_py27:
    executor: python27
    steps:
      - test_build
  test_build_win_py38: &test_build_win
    executor:
      name: win/default
      size: *resource_class
      shell: bash.exe
    environment:
      PYTHON_VERSION: 3.8.3
    working_directory: ~/repo
    steps:
      - checkout
      - test_build
  test_build_win_py37:
    <<: *test_build_win
    environment:
      PYTHON_VERSION: 3.7.7
  test_build_win_py36:
    <<: *test_build_win
    environment:
      PYTHON_VERSION: 3.6.8
  test_build_win_py35:
    <<: *test_build_win
    environment:
      PYTHON_VERSION: 3.5.4

  tracer:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: "tracer"

  opentracer:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^py..-opentracer'

  profile:
    <<: *contrib_job
    parallelism: 6
    steps:
      - run_tox_scenario:
          pattern: '^py..-profile'

  integration_agent5:
    <<: *machine_executor
    steps:
      - checkout
      - run: docker-compose up -d ddagent5
      - run:
          command: docker-compose logs -f
          background: true
      - run:
          command: ./scripts/ddtest scripts/run-tox-scenario '^py..-integration-v5'

  integration_agent:
    <<: *machine_executor
    steps:
      - checkout
      - run: docker-compose up -d ddagent
      - run:
          command: docker-compose logs -f
          background: true
      - run:
          command: ./scripts/ddtest scripts/run-tox-scenario '^py..-integration-latest'

  integration_testagent:
    <<: *machine_executor
    steps:
      - checkout
      - run: SNAPSHOT_CI=1 docker-compose up -d testagent
      - run:
          command: docker-compose logs -f
          background: true
      - run:
          environment:
            DD_TRACE_AGENT_URL: http://localhost:9126
          command: ./scripts/ddtest scripts/run-tox-scenario '^py..-integration-snapshot'

  vendor:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
    steps:
      - run_test:
          pattern: 'vendor'

  futures:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^futures_contrib-'

  boto:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: '^boto'  # run boto and botocore
          snapshot: true
          docker_services: "localstack"

  ddtracerun:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *redis_image
    steps:
      - run_test:
          store_coverage: false
          pattern: 'ddtracerun'

  test_logging:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: 'test_logging'

  asyncio:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^asyncio_contrib-'

  pylons:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^pylons_contrib-'

  aiohttp:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^aiohttp_contrib-'

  asgi:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: 'asgi$'

  tornado:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^tornado_contrib-'

  bottle:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^bottle_contrib\(_autopatch\)\?-'

  cassandra:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
        environment:
          CASS_DRIVER_NO_EXTENSIONS: 1
      - image: *cassandra_image
        environment:
          - MAX_HEAP_SIZE=512M
          - HEAP_NEWSIZE=256M
    steps:
      - run_tox_scenario:
          wait: cassandra
          pattern: '^cassandra_contrib-'

  celery:
    <<: *contrib_job
    parallelism: 6
    docker:
      - image: *ddtrace_dev_image
      - image: redis:4.0-alpine
    steps:
      - run_tox_scenario:
          pattern: '^celery_contrib-'

  cherrypy:
    <<: *machine_executor
    parallelism: 6
    steps:
      - run_test:
          pattern: 'cherrypy'
          snapshot: true

  consul:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *consul_image
    steps:
      - run_tox_scenario:
          pattern: '^consul_contrib-'

  dogpile_cache:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^dogpile_contrib-'

  elasticsearch:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: 'elasticsearch'
          snapshot: true
          docker_services: 'elasticsearch'

  falcon:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^falcon_contrib'

  django:
    <<: *machine_executor
    parallelism: 6
    steps:
      - run_test:
          pattern: 'django$'
          snapshot: true
          docker_services: "memcached redis"

  djangorestframework:
    <<: *machine_executor
    parallelism: 6
    steps:
      - run_test:
          pattern: 'djangorestframework'
          snapshot: true
          docker_services: "memcached redis"

  fastapi:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "fastapi"
          snapshot: true

  flask:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *redis_image
      - image: *memcached_image
    steps:
      - run_test:
          # Run both flask and flask_cache test suites
          pattern: 'flask(_cache)?'

  gevent:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^gevent_contrib-'

  httplib:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^httplib_contrib'

  grpc:
    <<: *machine_executor
    parallelism: 6
    steps:
      - run_test:
          pattern: "grpc"
          snapshot: true

  molten:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^molten_contrib-'

  mysqlconnector:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
    steps:
      - run_tox_scenario:
          wait: mysql
          pattern: '^mysql_contrib-.*-mysqlconnector'

  mysqlpython:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
    steps:
      - run_tox_scenario:
          wait: mysql
          pattern: '^mysqldb_contrib-.*-mysqlclient'

  mysqldb:
    executor: ddtrace_dev
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
    steps:
      - run_tox_scenario:
          wait: mysql
          pattern: '^mysqldb_contrib-.*-mysqldb'

  pymysql:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
    steps:
      - run_tox_scenario:
          wait: mysql
          pattern: '^pymysql_contrib-'

  pylibmc:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *memcached_image
    steps:
      - run_tox_scenario:
          pattern: '^pylibmc_contrib-'

  pytest:
    executor: ddtrace_dev
    steps:
      - run_tox_scenario:
          pattern: '^pytest_contrib'

  pymemcache:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *memcached_image
    steps:
      - run_tox_scenario:
          pattern: '^pymemcache_contrib\(_autopatch\)\?-'

  mongoengine:
    <<: *machine_executor
    parallelism: 1
    steps:
      - run_test:
          pattern: 'mongoengine'
          snapshot: true
          docker_services: 'mongo'

  pymongo:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *mongo_image
    steps:
      - run_test:
          pattern: "pymongo"

  pynamodb:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: "pynamodb"

  pyodbc:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
    steps:
      - run_tox_scenario:
          pattern: '^pyodbc_contrib-'

  pyramid:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^pyramid_contrib\(_autopatch\)\?-'

  requests:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *httpbin_local
    steps:
      - run_test:
          pattern: "requests"

  requestsgevent:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^requests_gevent_contrib-'

  sanic:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^sanic_contrib-'

  starlette:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "starlette"
          snapshot: true

  sqlalchemy:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *postgres_server
      - *mysql_server
    steps:
      - run_test:
          wait: postgres mysql
          pattern: "sqlalchemy"

  dbapi:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^dbapi_contrib-'

  psycopg:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: "psycopg"
          snapshot: true
          docker_services: "postgres"

  aiobotocore:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *moto_image
    steps:
      - run_tox_scenario:
          pattern: '^aiobotocore_contrib'

  aiopg:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *postgres_server
    steps:
      - run_tox_scenario:
          wait: postgres
          pattern: '^aiopg_contrib-'

  redis:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_tox_scenario_with_testagent:
          pattern: '^redis_contrib-'

  rediscluster:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *rediscluster_image
        environment:
          - IP=0.0.0.0
    steps:
      - run_tox_scenario:
          wait: rediscluster
          pattern: '^rediscluster_contrib-'

  urllib3:
    <<: *machine_executor
    steps:
      - checkout
      # `snapshot: true` will run `docker-compose logs -f` for us
      - run: docker-compose up -d httpbin_local
      - run_test:
          pattern: 'urllib3'
          snapshot: true

  vertica:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *vertica_image
        environment:
          - VP_TEST_USER=dbadmin
          - VP_TEST_PASSWORD=abc123
          - VP_TEST_DATABASE=docker
    steps:
      - run_tox_scenario:
          wait: vertica
          pattern: '^vertica_contrib-'

  wsgi:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "wsgi"
          snapshot: true

  kombu:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *rabbitmq_image
    steps:
      - run_tox_scenario:
          wait: rabbitmq
          pattern: '^kombu_contrib-'

  sqlite3:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^sqlite3_contrib-'

  benchmarks:
    <<: *contrib_job
    steps:
      - run_test:
          store_coverage: false
          pattern: '^benchmarks'

  deploy_master:
    # build the master branch releasing development docs and wheels
    <<: *machine_executor
    steps:
      - checkout
      - *pyenv-set-global
      - deploy_docs:
          s3_dir: *s3_dir_dev
      - deploy_wheels

  deploy_staging:
    <<: *machine_executor
    steps:
      - checkout
      - *pyenv-set-global
      - deploy_wheels

  jinja2:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^jinja2_contrib-'

  mako:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: 'mako'

  algoliasearch:
    <<: *contrib_job
    steps:
      - run_tox_scenario:
          pattern: '^algoliasearch_contrib-'

  build_docs:
    # deploy official documentation
    <<: *machine_executor
    steps:
      - checkout
      - *pyenv-set-global
      - build_docs
      - run:
          command: |
             mkdir -p /tmp/docs
             cp -r docs/_build/html/* /tmp/docs
      - store_artifacts:
          path: /tmp/docs

  deploy_docs:
    # deploy official documentation
    executor: python38
    steps:
      - checkout
      - deploy_docs:
          s3_dir: *s3_dir_prod

requires_pre_test: &requires_pre_test
  requires:
    - black
    - flake8
    - ccheck
    - mypy
    - build_base_venvs

requires_tests: &requires_tests
  requires:
    - build_docs
    - aiobotocore
    - aiohttp
    - aiopg
    - asyncio
    - algoliasearch
    - asgi
    - benchmarks
    - boto
    - bottle
    - cassandra
    - celery
    - cherrypy
    - consul
    - dbapi
    - ddtracerun
    - dogpile_cache
    - django
    - djangorestframework
    - elasticsearch
    - falcon
    - fastapi
    - flask
    - futures
    - gevent
    - grpc
    - httplib
    - integration_agent5
    - integration_agent
    - integration_testagent
    - vendor
    - profile
    - jinja2
    - kombu
    - mako
    - molten
    - mongoengine
    - mysqlconnector
    - mysqldb
    - mysqlpython
    - opentracer
    - psycopg
    - pylibmc
    - pylons
    - pymemcache
    - pymongo
    - pymysql
    - pynamodb
    - pyodbc
    - pyramid
    - pytest
    - redis
    - rediscluster
    - requests
    - sanic
    - sqlalchemy
    - sqlite3
    - starlette
    - test_build_alpine
    - test_build_py38
    - test_build_py37
    - test_build_py36
    - test_build_py35
    - test_build_py27
    - test_logging
    - tracer
    - tornado
    - urllib3
    - vertica
    - wsgi
    - build-docker-ci-image

workflows:
  version: 2

  deploy_docs:
    jobs:
      - build_docs:
          <<: *deploy_docs_filters
      - approve_docs_deployment:
          <<: *deploy_docs_filters
          type: approval
          requires:
            - build_docs
      - deploy_docs:
          <<: *deploy_docs_filters
          requires:
            - approve_docs_deployment
  test:
    jobs:
      # Jobs that should run before individual integration test suites
      - build_docs
      - black
      - flake8
      - mypy
      - ccheck
      - build_base_venvs

      # Test building the package
      - test_build_alpine: *requires_pre_test
      - test_build_py38: *requires_pre_test
      - test_build_py37: *requires_pre_test
      - test_build_py36: *requires_pre_test
      - test_build_py35: *requires_pre_test
      - test_build_py27: *requires_pre_test
      - test_build_win_py38: *requires_pre_test
      - test_build_win_py37: *requires_pre_test
      - test_build_win_py36: *requires_pre_test
      - test_build_win_py35: *requires_pre_test

      # Integration test suites
      - aiobotocore: *requires_pre_test
      - aiohttp: *requires_pre_test
      - aiopg: *requires_pre_test
      - asyncio: *requires_pre_test
      - algoliasearch: *requires_pre_test
      - asgi: *requires_pre_test
      - benchmarks: *requires_pre_test
      - boto: *requires_pre_test
      - bottle: *requires_pre_test
      - cassandra: *requires_pre_test
      - celery: *requires_pre_test
      - cherrypy: *requires_pre_test
      - consul: *requires_pre_test
      - dbapi: *requires_pre_test
      - ddtracerun: *requires_pre_test
      - django: *requires_pre_test
      - djangorestframework: *requires_pre_test
      - dogpile_cache: *requires_pre_test
      - elasticsearch: *requires_pre_test
      - falcon: *requires_pre_test
      - fastapi: *requires_pre_test
      - flask: *requires_pre_test
      - futures: *requires_pre_test
      - gevent: *requires_pre_test
      - grpc: *requires_pre_test
      - httplib: *requires_pre_test
      - integration_agent5: *requires_pre_test
      - integration_agent: *requires_pre_test
      - integration_testagent: *requires_pre_test
      - vendor: *requires_pre_test
      - profile: *requires_pre_test
      - jinja2: *requires_pre_test
      - kombu: *requires_pre_test
      - mako: *requires_pre_test
      - molten: *requires_pre_test
      - mongoengine: *requires_pre_test
      - mysqlconnector: *requires_pre_test
      - mysqldb: *requires_pre_test
      - mysqlpython: *requires_pre_test
      - opentracer: *requires_pre_test
      - psycopg: *requires_pre_test
      - pylibmc: *requires_pre_test
      - pylons: *requires_pre_test
      - pymemcache: *requires_pre_test
      - pymongo: *requires_pre_test
      - pymysql: *requires_pre_test
      - pynamodb: *requires_pre_test
      - pyodbc: *requires_pre_test
      - pyramid: *requires_pre_test
      - pytest: *requires_pre_test
      - redis: *requires_pre_test
      - rediscluster: *requires_pre_test
      - requests: *requires_pre_test
      - sanic: *requires_pre_test
      - starlette: *requires_pre_test
      - sqlalchemy: *requires_pre_test
      - sqlite3: *requires_pre_test
      - test_logging: *requires_pre_test
      - tornado: *requires_pre_test
      - tracer: *requires_pre_test
      - urllib3: *requires_pre_test
      - vertica: *requires_pre_test
      - wsgi: *requires_pre_test
      - build-docker-ci-image: *requires_pre_test
      - coverage_report: *requires_tests
      - deploy_master:
          filters:
            branches:
              only: master
      - deploy_staging:
          filters:
            branches:
              only: staging
