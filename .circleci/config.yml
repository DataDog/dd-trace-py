version: 2.1

default_resource_class: &default_resource_class medium
cimg_base_image: &cimg_base_image cimg/base:stable
python310_image: &python310_image cimg/python:3.10
ddtrace_dev_image: &ddtrace_dev_image ghcr.io/datadog/dd-trace-py/testrunner:latest
redis_image: &redis_image redis:4.0-alpine
memcached_image: &memcached_image memcached:1.5-alpine
cassandra_image: &cassandra_image cassandra:3.11.7
consul_image: &consul_image consul:1.6.0
moto_image: &moto_image palazzem/moto:1.0.1
mysql_image: &mysql_image mysql:5.7
postgres_image: &postgres_image postgres:12-alpine
mongo_image: &mongo_image mongo:3.6
httpbin_image: &httpbin_image kennethreitz/httpbin@sha256:2c7abc4803080c22928265744410173b6fea3b898872c01c5fd0f0f9df4a59fb
vertica_image: &vertica_image sumitchawla/vertica:latest
rabbitmq_image: &rabbitmq_image rabbitmq:3.7-alpine
testagent_image: &testagent_image ghcr.io/datadog/dd-apm-test-agent/ddapm-test-agent:v1.10.1

parameters:
  coverage:
    type: boolean
    default: false
  riot_run_latest:
    type: boolean
    default: false

orbs:
  win: circleci/windows@5.0

machine_executor: &machine_executor
  machine:
    image: ubuntu-2004:current
  environment:
    - BOTO_CONFIG: /dev/null
    # https://support.circleci.com/hc/en-us/articles/360045268074-Build-Fails-with-Too-long-with-no-output-exceeded-10m0s-context-deadline-exceeded-
    - PYTHONUNBUFFERED: 1
  steps:
    - &pyenv-set-global
      run:
        name: Set global pyenv
        command: |
          pyenv global 3.9.4

contrib_job_large: &contrib_job_large
  executor: ddtrace_dev
  parallelism: 12

contrib_job: &contrib_job
  executor: ddtrace_dev
  parallelism: 4

contrib_job_small: &contrib_job_small
  executor: ddtrace_dev_small
  parallelism: 2

commands:
  save_coverage:
    description: "Save coverage.py results to workspace"
    steps:
      - run: |
          set -ex
          mkdir coverage
          if [ -f .coverage ];
          then
            cp .coverage ./coverage/$CIRCLE_BUILD_NUM-$CIRCLE_JOB-$CIRCLE_NODE_INDEX.coverage
          fi
      - persist_to_workspace:
          root: coverage
          paths:
            - "*.coverage"
      - store_artifacts:
          path: coverage

  setup_riot:
    description: "Install riot"
    steps:
      # Make sure we install and run riot on Python 3
      - run: pip3 install riot==0.17.4

  save_pip_cache:
    description: "Save pip cache directory"
    steps:
      - save_cache:
          # DEV: Cache misses can still occur as venvs are not necessarily always created on the same node index
          key: pip-cache-xdg-{{ .Environment.CIRCLE_JOB }}-{{ .Environment.CIRCLE_NODE_INDEX }}-{{ checksum "riotfile.py" }}-{{ checksum "setup.py" }}
          paths:
            - "~/.cache/pip"

  restore_pip_cache:
    description: "Restore pip cache directory"
    steps:
      - restore_cache:
          key: pip-cache-xdg-{{ .Environment.CIRCLE_JOB }}-{{ .Environment.CIRCLE_NODE_INDEX }}-{{ checksum "riotfile.py" }}-{{ checksum "setup.py" }}

  start_docker_services:
    description: "Start Docker services"
    parameters:
      env:
        type: string
        default: ""
      services:
        type: string
        default: ""
    steps:
      # Retry pulls in case they fail
      - run: for i in {1..3}; do docker-compose pull -q << parameters.services >> && break || sleep 3; done
      - run: << parameters.env >> docker-compose up -d << parameters.services >>
      - run:
          command: docker-compose logs -f
          background: true

  run_test:
    description: "Run tests matching a pattern"
    parameters:
      pattern:
        type: string
        default: ""
      wait:
        type: string
        default: ""
      snapshot:
        type: boolean
        default: false
      docker_services:
        type: string
        default: ""
      store_coverage:
        type: boolean
        default: true
    steps:
      - attach_workspace:
          at: .
      - checkout
      - restore_pip_cache
      - when:
          condition:
              << parameters.snapshot >>
          steps:
            - setup_riot
            - start_docker_services:
                env: SNAPSHOT_CI=1
                services: testagent << parameters.docker_services >>
            - run:
                environment:
                  DD_TRACE_AGENT_URL: http://localhost:9126
                  RIOT_RUN_RECOMPILE_REQS: "<< pipeline.parameters.riot_run_latest >>"
                command: |
                  # Sort the hashes to ensure a consistent ordering/division between each node
                  riot list --hash-only '<<parameters.pattern>>' | sort | circleci tests split | xargs -n 1 -I {} ./scripts/ddtest riot -v run --exitfirst --pass-env -s {} $([[ << pipeline.parameters.coverage >> == false ]] && echo '--no-cov' )
                  ./scripts/check-diff ".riot/requirements/" "Changes detected after running riot. Consider deleting changed files, running scripts/compile-and-prune-test-requirements and committing the result."
      - unless:
          condition:
              << parameters.snapshot >>
          steps:
            - setup_riot
            - when:
                condition:
                  << parameters.wait >>
                steps:
                  - run:
                      name: "Waiting for << parameters.wait >>"
                      command: riot -v run 'wait' << parameters.wait >>
            - run:
                environment:
                  RIOT_RUN_RECOMPILE_REQS: "<< pipeline.parameters.riot_run_latest >>"
                command: |
                  # Sort the hashes to ensure a consistent ordering/division between each node
                  riot list --hash-only '<<parameters.pattern>>' | sort | circleci tests split | xargs -n 1 -I {} riot -v run --exitfirst --pass-env -s {} $([[ << pipeline.parameters.coverage >> == false ]] && echo '--no-cov' )
                  ./scripts/check-diff ".riot/requirements/" "Changes detected after running riot. Consider deleting changed files, running scripts/compile-and-prune-test-requirements and committing the result."
      - save_pip_cache
      - when:
          condition:
            and:
              - << pipeline.parameters.coverage >>
              - << parameters.store_coverage >>
          steps:
            - save_coverage
      - store_test_results:
          path: test-results
      - store_artifacts:
          path: test-results

executors:
  cimg_base:
    docker:
      - image: *cimg_base_image
    resource_class: medium
  python310:
    docker:
      - image: *python310_image
    resource_class: large
  ddtrace_dev:
    docker:
      - image: *ddtrace_dev_image
    resource_class: *default_resource_class
  ddtrace_dev_small:
    docker:
      - image: *ddtrace_dev_image
    resource_class: small

# Common configuration blocks as YAML anchors
# See: https://circleci.com/blog/circleci-hacks-reuse-yaml-in-your-circleci-config-with-yaml/
httpbin_local: &httpbin_local
  image: *httpbin_image
  name: httpbin.org

mysql_server: &mysql_server
  image: *mysql_image
  environment:
    - MYSQL_ROOT_PASSWORD=admin
    - MYSQL_PASSWORD=test
    - MYSQL_USER=test
    - MYSQL_DATABASE=test

postgres_server: &postgres_server
  image: *postgres_image
  environment:
    - POSTGRES_PASSWORD=postgres
    - POSTGRES_USER=postgres
    - POSTGRES_DB=postgres

testagent: &testagent
  image: *testagent_image
  environment:
    - LOG_LEVEL=WARNING
    - SNAPSHOT_DIR=/snapshots
    - SNAPSHOT_CI=1

jobs:
  pre_check:
    executor: python310
    steps:
      - checkout
      - setup_riot
      - run:
          name: "Formatting check"
          command: riot run -s fmt && git diff --exit-code
      - run:
          name: "Flake8 check"
          command: riot run -s flake8
      - run:
          name: "Slots check"
          command: riot run -s slotscheck
      - run:
          name: "Mypy check"
          command: riot run -s mypy
      - run:
          name: "cython-lint check"
          command: riot run -s cython-lint
      - run:
          name: "Codespell check"
          command: riot run -s codespell
      - run:
          name: "Run riotfile.py tests"
          command: riot run -s riot-helpers
      - run:
          name: "Test agent snapshot check"
          command: riot run -s snapshot-fmt && git diff --exit-code
      - run:
          name: "Run scripts/*.py tests"
          command: riot run -s scripts
      - run:
          name: "Running security analysis checks with bandit"
          command: riot -v run -s bandit

  ccheck:
    executor: cimg_base
    steps:
      - checkout
      - run: sudo apt-get update
      - run: sudo apt-get install --yes clang-format gcc-10 g++-10 python3 python3-setuptools python3-pip cppcheck
      - run: scripts/cformat.sh
      - run: scripts/cppcheck.sh
      - run: DD_COMPILE_DEBUG=1 DD_TESTING_RAISE=1 CC=gcc-10 CXX=g++-10 pip -vvv install .

  coverage_report:
    executor: python310
    steps:
      - when:
          condition:
            << pipeline.parameters.coverage >>
          steps:
            - checkout
            - attach_workspace:
                at: .
            - run: pip install coverage codecov diff_cover
            - run: ls -hal *.coverage
            # Combine all job coverage reports into one
            - run: coverage combine *.coverage
            # Upload coverage report to Codecov
            # DEV: Do not use the bash uploader, it cannot be trusted
            - run: codecov
            # Generate and save xml report
            # DEV: "--ignore-errors" to skip over files that are missing
            - run: coverage xml --ignore-errors
            - store_artifacts:
                path: coverage.xml
            # Generate and save JSON report
            # DEV: "--ignore-errors" to skip over files that are missing
            - run: coverage json --ignore-errors
            - store_artifacts:
                path: coverage.json
            # Print ddtrace/ report to stdout
            # DEV: "--ignore-errors" to skip over files that are missing
            - run: coverage report --ignore-errors --omit=tests/
            # Print tests/ report to stdout
            # DEV: "--ignore-errors" to skip over files that are missing
            - run: coverage report --ignore-errors --omit=ddtrace/
            # Print diff-cover report to stdout (compares against origin/1.x)
            - run: diff-cover --compare-branch $(git rev-parse --abbrev-ref origin/HEAD) coverage.xml
      - unless:
          condition:
            << pipeline.parameters.coverage >>
          steps:
            - run: echo "No coverage data collected. Set run workflow manually with pipeline parameter 'coverage=true'."


  build_base_venvs:
    resource_class: large
    docker:
      - image: *ddtrace_dev_image
    parallelism: 8
    steps:
      - checkout
      - setup_riot
      - run:
          name: "Generate base virtual environments."
          # DEV: riot list -i tracer lists all supported Python versions
          command: "riot list -i tracer | circleci tests split | xargs -I PY riot -v generate --python=PY"
      - persist_to_workspace:
          root: .
          paths:
            - "."

  appsec:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'appsec'
          snapshot: true
    
  aws_lambda:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'aws_lambda'
          snapshot: true

  internal:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: "internal"

  tracer:
    <<: *contrib_job_large
    steps:
      - run_test:
          pattern: "tracer"

  ci_visibility:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: "ci_visibility"

  sourcecode:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: "sourcecode"

  telemetry:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "telemetry"
          snapshot: true
          store_coverage: false

  debugger:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: "debugger"

  openai:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: 'openai'
          snapshot: true

  opentracer:
    <<: *contrib_job_large
    steps:
      - run_test:
          pattern: 'opentracer'

  opentelemetry:
    parallelism: 4
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'opentelemetry'
          snapshot: true

  profile:
    <<: *contrib_job
    resource_class: large
    # There are 32 jobs, so 2 per-node
    parallelism: 16
    steps:
      - run_test:
          store_coverage: false
          # We don't want to run the profile-diff venvs
          pattern: 'profile$'

  integration_agent:
    <<: *machine_executor
    parallelism: 2
    steps:
      - attach_workspace:
          at: .
      - checkout
      - setup_riot
      - start_docker_services:
          services: ddagent
      - run:
          environment:
            RIOT_RUN_RECOMPILE_REQS: "<< pipeline.parameters.riot_run_latest >>"
          command: |
            # Sort the hashes to ensure a consistent ordering/division between each node
            riot list --hash-only 'integration-latest' | sort | circleci tests split | xargs -n 1 -I {} ./scripts/ddtest riot -v run --pass-env -s {}
            ./scripts/check-diff ".riot/requirements/" "Changes detected after running riot. Consider deleting changed files, running scripts/compile-and-prune-test-requirements and committing the result."

  integration_testagent:
    <<: *machine_executor
    steps:
      - run_test:
          snapshot: true
          store_coverage: false
          pattern: 'integration-snapshot'

  vendor:
    <<: *contrib_job_small
    docker:
      - image: *ddtrace_dev_image
    steps:
      - run_test:
          pattern: 'vendor'

  boto:
    <<: *machine_executor
    parallelism: 6
    steps:
      - run_test:
          pattern: '^boto'  # run boto and botocore
          snapshot: true
          docker_services: "localstack"

  ddtracerun:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *redis_image
    steps:
      - run_test:
          store_coverage: false
          pattern: 'ddtracerun'

  test_logging:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: 'test_logging'

  stdlib:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    parallelism: 4
    steps:
      - run_test:
          pattern: 'asyncio$|sqlite3$|futures$|dbapi$|dbapi_async$'

  asyncpg:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'asyncpg'
          snapshot: true
          docker_services: 'postgres'

  pylons:
    <<: *contrib_job_small
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'pylons'

  aiohttp:
    <<: *machine_executor
    parallelism: 3
    steps:
      - run_test:
          pattern: 'aiohttp'  # includes aiohttp_jinja2
          snapshot: true
          docker_services: 'httpbin_local'

  asgi:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'asgi$'

  tornado:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'tornado'

  bottle:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'bottle'

  cassandra:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
        environment:
          CASS_DRIVER_NO_EXTENSIONS: 1
      - image: *cassandra_image
        environment:
          - MAX_HEAP_SIZE=512M
          - HEAP_NEWSIZE=256M
      - *testagent
    steps:
      - run_test:
          wait: cassandra
          pattern: 'cassandra'

  celery:
    <<: *contrib_job_large
    docker:
      - image: *ddtrace_dev_image
      - image: redis:4.0-alpine
      - image: *rabbitmq_image
      - *testagent
    steps:
      - run_test:
          pattern: 'celery'

  cherrypy:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: 'cherrypy'
          snapshot: true

  consul:
    <<: *contrib_job_small
    docker:
      - image: *ddtrace_dev_image
      - image: *consul_image
      - *testagent
    steps:
      - run_test:
          pattern: 'consul'

  dogpile_cache:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: 'dogpile_cache'

  elasticsearch:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: 'elasticsearch'
          snapshot: true
          docker_services: 'elasticsearch opensearch'

  falcon:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'falcon'

  django:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: 'django($|_celery)'
          snapshot: true
          docker_services: "memcached redis postgres"

  django_hosts:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'django_hosts$'
          snapshot: true

  djangorestframework:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: 'djangorestframework'
          snapshot: true
          docker_services: "memcached redis"

  fastapi:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: "fastapi"
          snapshot: true

  flask:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          # Run both flask and flask_cache test suites
          # TODO: Re-enable coverage for Flask tests
          store_coverage: false
          snapshot: true
          pattern: "flask"
          docker_services: "memcached redis"

  gevent:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'gevent'

  graphene:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "graphene"
          snapshot: true

  graphql:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "graphql"
          snapshot: true

  grpc:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: "grpc"
          snapshot: true

  gunicorn:
    <<: *machine_executor
    parallelism: 8
    steps:
      - run_test:
          pattern: "gunicorn"
          snapshot: true
          docker_services: 'ddagent'

  httplib:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "httplib"
          snapshot: true
          docker_services: 'httpbin_local'

  httpx:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: 'httpx'
          snapshot: true
          docker_services: 'httpbin_local'

  mariadb:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'mariadb$'
          snapshot: true
          docker_services: "mariadb"

  molten:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'molten'

  mysqlconnector:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
      - *testagent
    steps:
      - run_test:
          wait: mysql
          pattern: 'mysql'

  mysqlpython:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
      - *testagent
    steps:
      - run_test:
          wait: mysql
          pattern: 'mysqldb'

  pymysql:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *mysql_server
      - *testagent
    steps:
      - run_test:
          wait: mysql
          pattern: 'pymysql'

  pylibmc:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *memcached_image
      - *testagent
    steps:
      - run_test:
          pattern: 'pylibmc'

  pytest:
    <<: *contrib_job
    steps:
      - run_test:
          pattern: 'pytest$'

  asynctest:
    executor: ddtrace_dev
    steps:
      - run_test:
          pattern: 'asynctest$'

  pytestbdd:
    executor: ddtrace_dev
    steps:
      - run_test:
          pattern: 'pytest-bdd'

  pymemcache:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *memcached_image
      - *testagent
    steps:
      - run_test:
          pattern: "pymemcache"

  mongoengine:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'mongoengine'
          snapshot: true
          docker_services: 'mongo'

  pymongo:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *mongo_image
      - *testagent
    steps:
      - run_test:
          pattern: "pymongo"

  pynamodb:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: "pynamodb"

  pyodbc:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'pyodbc'

  pyramid:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'pyramid'
          snapshot: true

  requests:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
      - *httpbin_local
    steps:
      - run_test:
          pattern: "requests"

  sanic:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: "sanic"
          snapshot: true

  snowflake:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: "snowflake"
          snapshot: true

  starlette:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: "starlette"
          snapshot: true

  sqlalchemy:
    <<: *contrib_job_small
    docker:
      - image: *ddtrace_dev_image
      - *testagent
      - *postgres_server
      - *mysql_server
    steps:
      - run_test:
          wait: postgres mysql
          pattern: "sqlalchemy"

  psycopg:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: "psycopg$"
          snapshot: true
          docker_services: "postgres"

  psycopg2:
    <<: *machine_executor
    parallelism: 4
    steps:
      - run_test:
          pattern: "psycopg2"
          snapshot: true
          docker_services: "postgres"

  aiobotocore:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *moto_image
      - *testagent
    steps:
       - run_test:
          pattern: 'aiobotocore'

  aiomysql:
    <<: *machine_executor
    steps:
      - run_test:
          docker_services: 'mysql'
          wait: mysql
          pattern: 'aiomysql'
          snapshot: true

  aiopg:
    <<: *contrib_job_small
    docker:
      - image: *ddtrace_dev_image
      - *postgres_server
      - *testagent
    steps:
      - run_test:
          wait: postgres
          pattern: 'aiopg'

  aioredis:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          docker_services: 'redis'
          pattern: 'aioredis$'
          snapshot: true

  aredis:
    <<: *machine_executor
    steps:
      - run_test:
          docker_services: 'redis'
          pattern: 'aredis$'
          snapshot: true

  yaaredis:
    <<: *machine_executor
    steps:
      - run_test:
          docker_services: 'redis'
          pattern: 'yaaredis$'
          snapshot: true

  redis:
    <<: *machine_executor
    steps:
      - run_test:
          docker_services: 'redis rediscluster'
          pattern: 'redis$'
          snapshot: true

  rediscluster:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'rediscluster'
          docker_services: 'rediscluster'
          snapshot: true

  rq:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: "rq"
          snapshot: true
          docker_services: "redis"

  urllib3:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: 'urllib3'
          snapshot: true
          docker_services: "httpbin_local"

  vertica:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
      - image: *vertica_image
        environment:
          - VP_TEST_USER=dbadmin
          - VP_TEST_PASSWORD=abc123
          - VP_TEST_DATABASE=docker
    steps:
      - run_test:
          wait: vertica
          pattern: 'vertica'

  wsgi:
    <<: *machine_executor
    steps:
      - run_test:
          pattern: "wsgi"
          snapshot: true

  kafka:
    <<: *machine_executor
    parallelism: 2
    steps:
      - run_test:
          pattern: 'kafka'
          snapshot: true
          docker_services: 'kafka'

  kombu:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - image: *rabbitmq_image
      - *testagent
    steps:
      - run_test:
          wait: rabbitmq
          pattern: 'kombu'

  benchmarks:
    <<: *contrib_job_large
    steps:
      - run_test:
          store_coverage: false
          pattern: '^benchmarks'

  jinja2:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'jinja2'

  mako:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'mako'

  algoliasearch:
    <<: *contrib_job
    docker:
      - image: *ddtrace_dev_image
      - *testagent
    steps:
      - run_test:
          pattern: 'algoliasearch'

  build_docs:
    # build documentation and store as an artifact
    executor: ddtrace_dev
    steps:
      - setup_riot
      - checkout
      - run:
          command: |
             riot -v run docs
             mkdir -p /tmp/docs
             cp -r docs/_build/html/* /tmp/docs
      - store_artifacts:
          path: /tmp/docs

requires_pre_check: &requires_pre_check
  requires:
    - pre_check
    - ccheck

requires_base_venvs: &requires_base_venvs
  requires:
    - pre_check
    - ccheck
    - build_base_venvs

requires_tests: &requires_tests
  requires:
    - aiobotocore
    - aiohttp
    - aiomysql
    - aiopg
    - aioredis
    - asyncpg
    - algoliasearch
    - asgi
    - aws_lambda
    - benchmarks
    - boto
    - bottle
    - cassandra
    - celery
    - cherrypy
    - ci_visibility
    - consul
    - ddtracerun
    - dogpile_cache
    - django
    - django_hosts
    - djangorestframework
    - elasticsearch
    - falcon
    - fastapi
    - flask
    - gevent
    - graphql
    - graphene
    - grpc
    - gunicorn
    - httplib
    - httpx
    - internal
    - integration_agent
    - integration_testagent
    - vendor
    - profile
    - jinja2
    - kafka
    - kombu
    - mako
    - mariadb
    - molten
    - mongoengine
    - mysqlconnector
    - mysqlpython
    - openai
    - opentracer
    - opentelemetry
    - psycopg
    - psycopg2
    - pylibmc
    - pylons
    - pymemcache
    - pymongo
    - pymysql
    - pynamodb
    - pyodbc
    - pyramid
    - pytest
    - asynctest
    - pytestbdd
    - aredis
    - yaaredis
    - redis
    - rediscluster
    - requests
    - rq
    - sanic
    - snowflake
    - sqlalchemy
    - sourcecode 
    - starlette
    - stdlib
    - test_logging
    - tracer
    - telemetry
    - debugger
    - appsec
    - tornado
    - urllib3
    - vertica
    - wsgi

workflows:
  version: 2
  test: &workflow_test
    jobs:
      # Pre-checking before running all jobs
      - pre_check
      - ccheck

      # Build necessary base venvs for integration tests
      - build_base_venvs

      # Docs
      - build_docs: *requires_pre_check

      # Integration test suites
      - aiobotocore: *requires_base_venvs
      - aiohttp: *requires_base_venvs
      - aiomysql: *requires_base_venvs
      - aiopg: *requires_base_venvs
      - aioredis: *requires_base_venvs
      - asyncpg: *requires_base_venvs
      - algoliasearch: *requires_base_venvs
      - asgi: *requires_base_venvs
      - aws_lambda: *requires_base_venvs
      - benchmarks: *requires_base_venvs
      - boto: *requires_base_venvs
      - bottle: *requires_base_venvs
      - cassandra: *requires_base_venvs
      - celery: *requires_base_venvs
      - cherrypy: *requires_base_venvs
      - ci_visibility: *requires_base_venvs
      - consul: *requires_base_venvs
      - ddtracerun: *requires_base_venvs
      - django: *requires_base_venvs
      - django_hosts: *requires_base_venvs
      - djangorestframework: *requires_base_venvs
      - dogpile_cache: *requires_base_venvs
      - elasticsearch: *requires_base_venvs
      - falcon: *requires_base_venvs
      - fastapi: *requires_base_venvs
      - flask: *requires_base_venvs
      - gevent: *requires_base_venvs
      - graphene: *requires_base_venvs
      - graphql: *requires_base_venvs
      - grpc: *requires_base_venvs
      - gunicorn: *requires_base_venvs
      - httplib: *requires_base_venvs
      - httpx: *requires_base_venvs
      - integration_agent: *requires_base_venvs
      - integration_testagent: *requires_base_venvs
      - internal: *requires_base_venvs
      - vendor: *requires_base_venvs
      - profile: *requires_base_venvs
      - jinja2: *requires_base_venvs
      - kafka: *requires_base_venvs
      - kombu: *requires_base_venvs
      - mako: *requires_base_venvs
      - mariadb: *requires_base_venvs
      - molten: *requires_base_venvs
      - mongoengine: *requires_base_venvs
      - mysqlconnector: *requires_base_venvs
      - mysqlpython: *requires_base_venvs
      - openai: *requires_base_venvs
      - opentracer: *requires_base_venvs
      - opentelemetry: *requires_base_venvs
      - psycopg: *requires_base_venvs
      - psycopg2: *requires_base_venvs
      - pylibmc: *requires_base_venvs
      - pylons: *requires_base_venvs
      - pymemcache: *requires_base_venvs
      - pymongo: *requires_base_venvs
      - pymysql: *requires_base_venvs
      - pynamodb: *requires_base_venvs
      - pyodbc: *requires_base_venvs
      - pyramid: *requires_base_venvs
      - pytest: *requires_base_venvs
      - asynctest: *requires_base_venvs
      - pytestbdd: *requires_base_venvs
      - aredis: *requires_base_venvs
      - yaaredis: *requires_base_venvs
      - redis: *requires_base_venvs
      - rediscluster: *requires_base_venvs
      - requests: *requires_base_venvs
      - rq: *requires_base_venvs
      - sanic: *requires_base_venvs
      - snowflake: *requires_base_venvs
      - starlette: *requires_base_venvs
      - stdlib: *requires_base_venvs
      - sqlalchemy: *requires_base_venvs
      - sourcecode: *requires_base_venvs
      - test_logging: *requires_base_venvs
      - tornado: *requires_base_venvs
      - tracer: *requires_base_venvs
      - telemetry: *requires_base_venvs
      - debugger: *requires_base_venvs
      - appsec: *requires_base_venvs
      - urllib3: *requires_base_venvs
      - vertica: *requires_base_venvs
      - wsgi: *requires_base_venvs
      # Final reports
      - coverage_report: *requires_tests
