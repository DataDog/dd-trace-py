version: 2


# Common configuration blocks as YAML anchors
# See: https://circleci.com/blog/circleci-hacks-reuse-yaml-in-your-circleci-config-with-yaml/
httpbin_local: &httpbin_local
  image: kennethreitz/httpbin@sha256:2c7abc4803080c22928265744410173b6fea3b898872c01c5fd0f0f9df4a59fb
  name: httpbin.org
test_runner: &test_runner
  image: datadog/docker-library:ddtrace_py
  env:
    TOX_SKIP_DIST: True
restore_cache_step: &restore_cache_step
  restore_cache:
    keys:
        # In the cache key:
        #   - .Environment.CIRCLE_JOB: We do separate tox environments by job name, so caching and restoring is
        #                              much faster.
        #   - .Environment.CACHE_EXPIRE_HASH: Typically CircleCI discard caches every ~60days. If we see any strange
        #                              behavior in tests and we want to run a build in a clean environment, we should
        #                              still be able to do it. In order to achieve this we can change the value of the
        #                              CACHE_EXPIRE_HASH in our CircleCI's repo settings. Please use the format
        #                              'YYYY-MM-DD'. This way a new push on the branch is not required.
        - tox-cache-{{ .Environment.CIRCLE_JOB }}-{{ .Environment.CACHE_EXPIRE_HASH }}
resource_class: &resource_class small
save_cache_step: &save_cache_step
  save_cache:
    key: tox-cache-{{ .Environment.CIRCLE_JOB }}-{{ .Environment.CACHE_EXPIRE_HASH }}
    paths:
      - .tox
deploy_docs_filters: &deploy_docs_filters
  filters:
    tags:
      only: /(^docs$)|(^v[0-9]+(\.[0-9]+)*$)/
    branches:
      ignore: /.*/


jobs:
  flake8:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'flake8' --result-json /tmp/flake8.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - flake8.results
      - *save_cache_step

  # Test that we can build the package properly and package long description will render
  test_build:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      # Install required dependencies
      # DEV: `pyopenssl` needed until the following PR is released
      #      https://github.com/pypa/twine/pull/447
      - run: pip install twine readme_renderer[md] pyopenssl
      # Ensure we didn't cache from previous runs
      - run: rm -rf dist/
      # Ensure package will build
      - run: python setup.py sdist
      # Ensure package long description is valid and will render
      # https://github.com/pypa/twine/tree/6c4d5ecf2596c72b89b969ccc37b82c160645df8#twine-check
      - run: twine check dist/*
      - *save_cache_step

  tracer:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e '{py27,py34,py35,py36}-tracer' --result-json /tmp/tracer.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - tracer.results
      - *save_cache_step

  opentracer:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e '{py27,py34,py35,py36}-opentracer' --result-json /tmp/opentracer.results
      - run: tox -e '{py34,py35,py36}-opentracer_asyncio' --result-json /tmp/opentracer-asyncio.results
      - run: tox -e '{py34,py35,py36}-opentracer_tornado-tornado{40,41,42,43,44}' --result-json /tmp/opentracer-tornado.results
      - run: tox -e '{py27}-opentracer_gevent-gevent{10}' --result-json /tmp/opentracer-gevent.1.results
      - run: tox -e '{py27,py34,py35,py36}-opentracer_gevent-gevent{11,12}' --result-json /tmp/opentracer-gevent.2.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - opentracer.results
            - opentracer-asyncio.results
            - opentracer-tornado.results
            - opentracer-gevent.1.results
            - opentracer-gevent.2.results
      - *save_cache_step

  integration:
    docker:
      - <<: *test_runner
        env:
          TOX_SKIP_DIST: True
          TEST_DATADOG_INTEGRATION: 1
      - image: datadog/docker-dd-agent
        env:
            - DD_APM_ENABLED=true
            - DD_BIND_HOST=0.0.0.0
            - DD_API_KEY=invalid_key_but_this_is_fine
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e '{py27,py34,py35,py36}-integration' --result-json /tmp/integration.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - integration.results

      - *save_cache_step

  futures:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'futures_contrib-{py27}-futures{30,31,32}' --result-json /tmp/futures.1.results
      - run: tox -e 'futures_contrib-{py34,py35,py36}' --result-json /tmp/futures.2.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - futures.1.results
            - futures.2.results
      - *save_cache_step

  boto:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'boto_contrib-{py27,py34}-boto' --result-json /tmp/boto.1.results
      - run: tox -e 'botocore_contrib-{py27,py34,py35,py36}-botocore' --result-json /tmp/boto.2.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - boto.1.results
            - boto.2.results
      - *save_cache_step

  ddtracerun:
    docker:
      - *test_runner
      - image: redis:4.0-alpine
    environment:
      TOX_SKIP_DIST: False
    resource_class: *resource_class
    steps:
      - checkout
      - run: tox -e '{py27,py34,py35,py36}-ddtracerun' --result-json /tmp/ddtracerun.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - ddtracerun.results

  test_utils:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e '{py27,py34,py35,py36}-test_utils' --result-json /tmp/test_utils.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - test_utils.results
      - *save_cache_step

  test_logging:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e '{py27,py34,py35,py36}-test_logging' --result-json /tmp/test_logging.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - test_logging.results
      - *save_cache_step

  asyncio:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'asyncio_contrib-{py34,py35,py36}' --result-json /tmp/asyncio.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - asyncio.results
      - *save_cache_step

  pylons:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'pylons_contrib-{py27}-pylons{096,097,010,10}' --result-json /tmp/pylons.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - pylons.results
      - *save_cache_step

  aiohttp:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'aiohttp_contrib-{py34,py35,py36}-aiohttp{12,13,20,21,22}-aiohttp_jinja{012,013}-yarl' --result-json /tmp/aiohttp.1.results
      - run: tox -e 'aiohttp_contrib-{py34,py35,py36}-aiohttp{23}-aiohttp_jinja{015}-yarl10' --result-json /tmp/aiohttp.2.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - aiohttp.1.results
            - aiohttp.2.results
      - *save_cache_step

  tornado:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'tornado_contrib-{py27,py34,py35,py36}-tornado{40,41,42,43,44,45}' --result-json /tmp/tornado.1.results
      - run: tox -e 'tornado_contrib-{py27}-tornado{40,41,42,43,44,45}-futures{30,31,32}' --result-json /tmp/tornado.2.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - tornado.1.results
            - tornado.2.results
      - *save_cache_step

  bottle:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'bottle_contrib{,_autopatch}-{py27,py34,py35,py36}-bottle{11,12}-webtest' --result-json /tmp/bottle.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - bottle.results
      - *save_cache_step

  cassandra:
    docker:
      - <<: *test_runner
        env:
          TOX_SKIP_DIST: True
          CASS_DRIVER_NO_EXTENSIONS: 1
      - image: cassandra:3.11
        env:
          - MAX_HEAP_SIZE=1024M
          - HEAP_NEWSIZE=400M
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e wait cassandra
      - run: tox -e 'cassandra_contrib-{py27,py34,py35,py36}-cassandra{35,36,37,38,315}' --result-json /tmp/cassandra.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - cassandra.results
      - *save_cache_step

  celery:
    docker:
      - <<: *test_runner
        env:
          TOX_SKIP_DIST: False
      - image: redis:4.0-alpine
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'celery_contrib-{py27,py34,py35,py36}-celery{31,40,41,42}-redis{210}' --result-json /tmp/celery.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - celery.results
      - *save_cache_step

  elasticsearch:
    docker:
      - *test_runner
      - image: elasticsearch:2.3
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run:
          command: |
            tox -e 'elasticsearch_contrib-{py27,py34,py35,py36}-elasticsearch{16,17,18,23,24,51,52,53,54,63}' \
                -e 'elasticsearch_contrib-{py27,py34,py35,py36}-elasticsearch1{100}' \
                -e 'elasticsearch_contrib-{py27,py34,py35,py36}-elasticsearch2{50}' \
                -e 'elasticsearch_contrib-{py27,py34,py35,py36}-elasticsearch5{50}' \
                --result-json /tmp/elasticsearch.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - elasticsearch.results
      - *save_cache_step

  falcon:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'falcon_contrib{,_autopatch}-{py27,py34,py35,py36}-falcon{10,11,12,13,14}' --result-json /tmp/falcon.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - falcon.results
      - *save_cache_step

  django:
    docker:
      - *test_runner
      - image: redis:4.0-alpine
      - image: memcached:1.5-alpine
      - image: datadog/docker-dd-agent
        env:
            - DD_APM_ENABLED=true
            - DD_BIND_HOST=0.0.0.0
            - DD_API_KEY=invalid_key_but_this_is_fine
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'django_contrib{,_autopatch}-{py27,py34,py35,py36}-django{18,111}-djangopylibmc06-djangoredis45-pylibmc-redis{210}-memcached' --result-json /tmp/django.1.results
      - run: tox -e 'django_drf_contrib-{py27,py34,py35,py36}-django{111}-djangorestframework{34,37,38}' --result-json /tmp/django.2.results
      - run: tox -e 'django_contrib{,_autopatch}-{py34,py35,py36}-django{200}-djangopylibmc06-djangoredis45-pylibmc-redis{210}-memcached' --result-json /tmp/django.3.results
      - run: tox -e 'django_drf_contrib-{py34,py35,py36}-django{200}-djangorestframework{37,38}' --result-json /tmp/django.4.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - django.1.results
            - django.2.results
            - django.3.results
            - django.4.results
      - *save_cache_step

  flask:
    docker:
      - *test_runner
      - image: redis:4.0-alpine
      - image: memcached:1.5-alpine
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'flask_contrib-{py27,py34,py35,py36}-flask{010,011,012,10}-blinker' --result-json /tmp/flask.1.results
      - run: TOX_SKIP_DIST=False tox -e 'flask_contrib_autopatch-{py27,py34,py35,py36}-flask{010,011,012,10}-blinker' --result-json /tmp/flask.2.results
      - run: tox -e 'flask_contrib-{py27}-flask{09}-blinker' --result-json /tmp/flask.3.results
      - run: TOX_SKIP_DIST=False tox -e 'flask_contrib_autopatch-{py27}-flask{09}-blinker' --result-json /tmp/flask.4.results
      - run: tox -e 'flask_cache_contrib-{py27,py34,py35,py36}-flask{010,011,012}-flaskcache{013}-memcached-redis{210}-blinker' --result-json /tmp/flask.5.results
      - run: TOX_SKIP_DIST=False tox -e 'flask_cache_contrib_autopatch-{py27,py34,py35,py36}-flask{010,011,012}-flaskcache{013}-memcached-redis{210}-blinker' --result-json /tmp/flask.6.results
      - run: tox -e 'flask_cache_contrib-{py27}-flask{010,011}-flaskcache{012}-memcached-redis{210}-blinker' --result-json /tmp/flask.7.results
      - run: TOX_SKIP_DIST=False tox -e 'flask_cache_contrib_autopatch-{py27}-flask{010,011}-flaskcache{012}-memcached-redis{210}-blinker' --result-json /tmp/flask.8.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - flask.1.results
            - flask.2.results
            - flask.3.results
            - flask.4.results
            - flask.5.results
            - flask.6.results
            - flask.7.results
            - flask.8.results
      - *save_cache_step

  gevent:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'gevent_contrib-{py27,py34,py35,py36}-gevent{11,12,13}' --result-json /tmp/gevent.1.results
      - run: tox -e 'gevent_contrib-{py27}-gevent{10}' --result-json /tmp/gevent.2.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - gevent.1.results
            - gevent.2.results
      - *save_cache_step

  httplib:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'httplib_contrib-{py27,py34,py35,py36}' --result-json /tmp/httplib.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - httplib.results
      - *save_cache_step

  grpc:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'grpc_contrib-{py27,py34,py35,py36}-grpc' --result-json /tmp/grpc.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - grpc.results
      - *save_cache_step

  molten:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'molten_contrib-{py36}-molten{070,072}' --result-json /tmp/molten.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - molten.results
      - *save_cache_step

  mysqlconnector:
    docker:
      - *test_runner
      - image: mysql:5.7
        env:
            - MYSQL_ROOT_PASSWORD=admin
            - MYSQL_PASSWORD=test
            - MYSQL_USER=test
            - MYSQL_DATABASE=test
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'wait' mysql
      - run: tox -e 'mysql_contrib-{py27,py34,py35,py36}-mysqlconnector{21}' --result-json /tmp/mysqlconnector.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - mysqlconnector.results
      - *save_cache_step

  mysqlpython:
    docker:
      - *test_runner
      - image: mysql:5.7
        env:
            - MYSQL_ROOT_PASSWORD=admin
            - MYSQL_PASSWORD=test
            - MYSQL_USER=test
            - MYSQL_DATABASE=test
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'wait' mysql
      - run: tox -e 'mysqldb_contrib-{py27,py34,py35,py36}-mysqlclient{13}' --result-json /tmp/mysqlpython.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - mysqlpython.results
      - *save_cache_step

  mysqldb:
    docker:
      - *test_runner
      - image: mysql:5.7
        env:
            - MYSQL_ROOT_PASSWORD=admin
            - MYSQL_PASSWORD=test
            - MYSQL_USER=test
            - MYSQL_DATABASE=test
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'wait' mysql
      - run: tox -e 'mysqldb_contrib-{py27}-mysqldb{12}' --result-json /tmp/mysqldb.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - mysqldb.results
      - *save_cache_step

  pymysql:
    docker:
      - *test_runner
      - image: mysql:5.7
        env:
            - MYSQL_ROOT_PASSWORD=admin
            - MYSQL_PASSWORD=test
            - MYSQL_USER=test
            - MYSQL_DATABASE=test
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'wait' mysql
      - run: tox -e 'pymysql_contrib-{py27,py34,py35,py36}-pymysql{07,08,09}' --result-json /tmp/pymysql.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - pymysql.results
      - *save_cache_step

  pylibmc:
    docker:
      - *test_runner
      - image: memcached:1.5-alpine
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'pylibmc_contrib-{py27,py34,py35,py36}-pylibmc{140,150}' --result-json /tmp/pylibmc.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - pylibmc.results
      - *save_cache_step

  pymemcache:
    docker:
      - *test_runner
      - image: memcached:1.5-alpine
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'pymemcache_contrib{,_autopatch}-{py27,py34,py35,py36}-pymemcache{130,140}' --result-json /tmp/pymemcache.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - pymemcache.results
      - *save_cache_step

  mongoengine:
    docker:
      - *test_runner
      - image: mongo:3.6
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'mongoengine_contrib-{py27,py34,py35,py36}-mongoengine{015}' --result-json /tmp/mongoengine.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - mongoengine.results
      - *save_cache_step

  pymongo:
    docker:
      - *test_runner
      - image: mongo:3.6
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'pymongo_contrib-{py27,py34,py35,py36}-pymongo{30,31,32,33,34,36}-mongoengine{015}' --result-json /tmp/pymongo.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - pymongo.results
      - *save_cache_step

  pyramid:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'pyramid_contrib{,_autopatch}-{py27,py34,py35,py36}-pyramid{17,18,19}-webtest' --result-json /tmp/pyramid.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - pyramid.results
      - *save_cache_step

  requests:
    docker:
      - *test_runner
      - *httpbin_local
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'requests_contrib{,_autopatch}-{py27,py34,py35,py36}-requests{208,209,210,211,212,213,219}' --result-json /tmp/requests.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - requests.results
      - *save_cache_step

  requestsgevent:
    docker:
      - *test_runner
      - *httpbin_local
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'requests_gevent_contrib-{py36}-requests{208,209,210,211,212,213,219}-gevent{12,13}' --result-json /tmp/requestsgevent.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - requestsgevent.results
      - *save_cache_step

  sqlalchemy:
    docker:
      - *test_runner
      - image: postgres:10.5-alpine
        env:
            - POSTGRES_PASSWORD=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_DB=postgres
      - image: mysql:5.7
        env:
            - MYSQL_ROOT_PASSWORD=admin
            - MYSQL_PASSWORD=test
            - MYSQL_USER=test
            - MYSQL_DATABASE=test
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'wait' postgres mysql
      - run: tox -e 'sqlalchemy_contrib-{py27,py34,py35,py36}-sqlalchemy{10,11,12}-psycopg2{27}-mysqlconnector{21}' --result-json /tmp/sqlalchemy.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - sqlalchemy.results
      - *save_cache_step

  dbapi:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'dbapi_contrib-{py27,py34,py35,py36}' --result-json /tmp/dbapi.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - dbapi.results
      - *save_cache_step

  psycopg:
    docker:
      - *test_runner
      - image: postgres:10.5-alpine
        env:
            - POSTGRES_PASSWORD=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_DB=postgres
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'wait' postgres
      - run: tox -e 'psycopg_contrib-{py27,py34,py35,py36}-psycopg2{24,25,26,27}' --result-json /tmp/psycopg.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - psycopg.results
      - *save_cache_step

  aiobotocore:
    docker:
      - *test_runner
      - image: palazzem/moto:1.0.1
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'aiobotocore_contrib-{py34,py35,py36}-aiobotocore{02,03,04}' --result-json /tmp/aiobotocore.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - aiobotocore.results
      - *save_cache_step

  aiopg:
    docker:
      - *test_runner
      - image: postgres:10.5-alpine
        env:
            - POSTGRES_PASSWORD=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_DB=postgres
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'wait' postgres
      - run: tox -e 'aiopg_contrib-{py34,py35,py36}-aiopg{012,015}' --result-json /tmp/aiopg.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - aiopg.results
      - *save_cache_step

  redis:
    docker:
      - *test_runner
      - image: redis:4.0-alpine
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'redis_contrib-{py27,py34,py35,py36}-redis{26,27,28,29,210,300}' --result-json /tmp/redis.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - redis.results
      - *save_cache_step

  rediscluster:
    docker:
      - *test_runner
      - image: grokzen/redis-cluster:4.0.9
        env:
          - IP=0.0.0.0
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e wait rediscluster
      - run: tox -e 'rediscluster_contrib-{py27,py34,py35,py36}-rediscluster{135,136}-redis210' --result-json /tmp/rediscluster.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - rediscluster.results
      - *save_cache_step

  vertica:
    docker:
      - *test_runner
      - image: sumitchawla/vertica
        env:
          - VP_TEST_USER=dbadmin
          - VP_TEST_PASSWORD=abc123
          - VP_TEST_DATABASE=docker
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e wait vertica
      - run: tox -e 'vertica_contrib-{py27,py34,py35,py36}-vertica{060,070}' --result-json /tmp/vertica.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - vertica.results
      - *save_cache_step

  kombu:
    docker:
    - *test_runner
    - image: rabbitmq:3.7-alpine
    resource_class: *resource_class
    steps:
    - checkout
    - *restore_cache_step
    - run: tox -e wait rabbitmq
    - run: tox -e 'kombu_contrib-{py27,py34,py35,py36}-kombu{40,41,42}' --result-json /tmp/kombu.results
    - persist_to_workspace:
        root: /tmp
        paths:
        - kombu.results
    - *save_cache_step

  sqlite3:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'sqlite3_contrib-{py27,py34,py35,py36}-sqlite3' --result-json /tmp/sqlite3.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - sqlite3.results
      - *save_cache_step

  msgpack:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'msgpack_contrib-{py27,py34}-msgpack{03,04,05}' --result-json /tmp/msgpack.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - msgpack.results
      - *save_cache_step

  unit_tests:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'unit_tests-{py27,py34,py35,py36}' --result-json /tmp/unit_tests.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - unit_tests.results
      - *save_cache_step

  deploy_dev:
    # build only the nightly package
    docker:
      - image: circleci/python:3.6
    resource_class: *resource_class
    steps:
      - checkout
      - run: sudo apt-get -y install rake
      - run: sudo pip install mkwheelhouse sphinx awscli wrapt
      - run: S3_DIR=trace-dev rake release:docs
      - run: VERSION_SUFFIX=$CIRCLE_BRANCH$CIRCLE_BUILD_NUM S3_DIR=trace-dev rake release:wheel

  deploy_experimental:
    # build the *-dev branch releasing development docs
    docker:
      - image: circleci/python:3.6
    resource_class: *resource_class
    steps:
      - checkout
      - run: sudo apt-get -y install rake
      - run: sudo pip install mkwheelhouse sphinx awscli wrapt
      - run: VERSION_SUFFIX=$CIRCLE_BRANCH$CIRCLE_BUILD_NUM S3_DIR=trace-dev rake release:wheel

  jinja2:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'jinja2_contrib-{py27,py34,py35,py36}-jinja{27,28,29,210}' --result-json /tmp/jinja2.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - jinja2.results
      - *save_cache_step

  mako:
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - checkout
      - *restore_cache_step
      - run: tox -e 'mako_contrib-{py27,py34,py35,py36}-mako{010,100}' --result-json /tmp/mako.results
      - persist_to_workspace:
          root: /tmp
          paths:
            - mako.results
      - *save_cache_step

  build_docs:
    # deploy official documentation
    docker:
      - image: circleci/python:3.6
    resource_class: *resource_class
    steps:
      - checkout
      - run: sudo apt-get -y install rake
      # Sphinx 1.7.5 is required otherwise docs are not properly built
      - run: sudo pip install mkwheelhouse sphinx==1.7.5 wrapt
      - run: rake docs
      - run:
          command: |
             mkdir -p /tmp/docs
             cp -r docs/_build/html/* /tmp/docs
      - store_artifacts:
          path: /tmp/docs

  deploy_to_s3:
    # deploy official documentation
    docker:
      - image: circleci/python:3.6
    resource_class: *resource_class
    steps:
      - checkout
      - run: sudo apt-get -y install rake
      # Sphinx 1.7.5 is required otherwise docs are not properly built
      - run: sudo pip install mkwheelhouse sphinx==1.7.5 awscli wrapt
      - run: S3_DIR=trace rake release:docs

  wait_all_tests:
    # this step ensures all `tox` environments are properly executed
    docker:
      - *test_runner
    resource_class: *resource_class
    steps:
      - attach_workspace:
          at: /tmp/workspace
      - checkout
      - run: ls /tmp/workspace/*
        # debug: shows how many time each test was executed
      - run: jq -s ".[]|.testenvs|keys|.[]" /tmp/workspace/* | grep -v GLOB | sed 's/"//g' | sort | uniq -c | sort -rn
        # list all executed test
      - run: jq -s ".[]|.testenvs|keys|.[]" /tmp/workspace/* | grep -v GLOB | sed 's/"//g' | sort | uniq | tee all_executed_tests
        # list all tests in tox.ini
      - run: tox -l | grep -v "^wait$" | sort > all_tests
        # checks that all tests were executed
      - run: diff all_tests all_executed_tests


workflows:
  version: 2

  deploy_docs:
    jobs:
      - build_docs:
          <<: *deploy_docs_filters
      - approve_docs_deployment:
          <<: *deploy_docs_filters
          type: approval
          requires:
            - build_docs
      - deploy_to_s3:
          <<: *deploy_docs_filters
          requires:
            - approve_docs_deployment
  test:
    jobs:
      - build_docs
      - flake8
      - test_build
      - aiobotocore:
          requires:
            - flake8
      - aiohttp:
          requires:
            - flake8
      - aiopg:
          requires:
            - flake8
      - asyncio:
          requires:
            - flake8
      - boto:
          requires:
            - flake8
      - bottle:
          requires:
            - flake8
      - cassandra:
          requires:
            - flake8
      - celery:
          requires:
            - flake8
      - dbapi:
          requires:
            - flake8
      - ddtracerun:
          requires:
            - flake8
      - django:
          requires:
            - flake8
      - elasticsearch:
          requires:
            - flake8
      - falcon:
          requires:
            - flake8
      - flask:
          requires:
            - flake8
      - futures:
          requires:
            - flake8
      - gevent:
          requires:
            - flake8
      - grpc:
          requires:
            - flake8
      - httplib:
          requires:
            - flake8
      - integration:
          requires:
            - flake8
      - jinja2:
          requires:
            - flake8
      - kombu:
          requires:
            - flake8
      - mako:
          requires:
            - flake8
      - molten:
          requires:
            - flake8
      - mongoengine:
          requires:
            - flake8
      - msgpack:
          requires:
            - flake8
      - mysqlconnector:
          requires:
            - flake8
      - mysqldb:
          requires:
            - flake8
      - mysqlpython:
          requires:
            - flake8
      - opentracer:
          requires:
            - flake8
      - psycopg:
          requires:
            - flake8
      - pylibmc:
          requires:
            - flake8
      - pylons:
          requires:
            - flake8
      - pymemcache:
          requires:
            - flake8
      - pymongo:
          requires:
            - flake8
      - pymysql:
          requires:
            - flake8
      - pyramid:
          requires:
            - flake8
      - redis:
          requires:
            - flake8
      - rediscluster:
          requires:
            - flake8
      - requests:
          requires:
            - flake8
      - requestsgevent:
          requires:
            - flake8
      - sqlalchemy:
          requires:
            - flake8
      - sqlite3:
          requires:
            - flake8
      - test_utils:
          requires:
            - flake8
      - test_logging:
          requires:
            - flake8
      - tornado:
          requires:
            - flake8
      - tracer:
          requires:
            - flake8
      - unit_tests:
          requires:
            - flake8
      - vertica:
          requires:
            - flake8
      - wait_all_tests:
          requires:
            # Initial jobs
            - build_docs
            - flake8
            - test_build

            # flake8 dependent jobs
            - aiobotocore
            - aiohttp
            - aiopg
            - asyncio
            - boto
            - bottle
            - cassandra
            - celery
            - dbapi
            - ddtracerun
            - django
            - elasticsearch
            - falcon
            - flask
            - futures
            - gevent
            - grpc
            - httplib
            - integration
            - jinja2
            - kombu
            - mako
            - molten
            - mongoengine
            - msgpack
            - mysqlconnector
            - mysqldb
            - mysqlpython
            - opentracer
            - psycopg
            - pylibmc
            - pylons
            - pymemcache
            - pymongo
            - pymysql
            - pyramid
            - redis
            - rediscluster
            - requests
            - requestsgevent
            - sqlalchemy
            - sqlite3
            - test_utils
            - test_logging
            - tornado
            - tracer
            - unit_tests
            - vertica
      - deploy_dev:
          requires:
            - wait_all_tests
          filters:
            branches:
              only: /(master)/
      - deploy_experimental:
          requires:
            - wait_all_tests
          filters:
            branches:
              only: /(.*-dev)/
