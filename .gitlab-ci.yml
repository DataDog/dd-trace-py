stages:
  - package
  - tests
  - fuzz
  - shared-pipeline
  - benchmarks
  - release

variables:
  REPO_LANG: python # "python" is used everywhere rather than "py"
  REPO_NOTIFICATION_CHANNEL: "#apm-python-release"
  RELEASE_ALLOW_TEST_FAILURES: false
  RELEASE_ALLOW_BENCHMARK_FAILURES: false
  # VPA Template configuration
  DD_VPA_TEMPLATE: "vpa-template-cpu-p70-10percent-2x-oom-min-cap"
  # CI_DEBUG_SERVICES: "true"
  # Automatically managed, use scripts/update-system-tests-version to update
  SYSTEM_TESTS_REF: "8c86fdebd50a46ca9eccb1c8323b6295040db04b"

  # Profiling native build image (built from dd/images/dd-trace-py/profiling_native)
  PROFILING_NATIVE_IMAGE: "registry.ddbuild.io/dd-trace-py:v96371602-f288245-profiling_native"
  # Repository URL for CI Visibility
  DD_GIT_REPOSITORY_URL: "https://github.com/DataDog/dd-trace-py.git"

default:
  interruptible: true


.is_main: $CI_COMMIT_REF_NAME == "main"
.is_release: $CI_COMMIT_TAG =~ /^v[0-9]+\.[0-9]+\.[0-9]+(rc[0-9]+)?$/
.is_release_branch: $CI_COMMIT_BRANCH =~ /^[0-9]+\.[0-9]+$/
.is_gh_merge_queue: $CI_COMMIT_BRANCH =~ /^gh-readonly-queue\/.*$/
.is_devflow_merge_queue: $CI_COMMIT_BRANCH =~ /^mq-working-branch-.*$/

# trigger new commit cancel
workflow:
  auto_cancel:
    on_new_commit: interruptible
  rules:
    - if: !reference [.is_main]
      auto_cancel:
        on_new_commit: none
    - if: !reference [.is_release_branch]
      auto_cancel:
        on_new_commit: none
    - when: always

include:
  - local: ".gitlab/ci-templates.yml" # Reusable templates (setup_ci_deps, etc.)
  - local: ".gitlab/one-pipeline.locked.yml"
  - local: ".gitlab/services.yml" # Include early so others can use the definitions
  - local: ".gitlab/package.yml"
  - local: ".gitlab/release.yml"
  - local: ".gitlab/testrunner.yml"
  - local: ".gitlab/multi-os-tests.yml"
  - local: ".gitlab/benchmarks/serverless.yml"
  - local: ".gitlab/native.yml"
  - local: ".gitlab/fuzz.yml"

tests-gen:
  stage: tests
  extends: .testrunner
  variables:
    UNPIN_DEPENDENCIES: "${UNPIN_DEPENDENCIES:-}"
    NIGHTLY_BUILD: "${NIGHTLY_BUILD:-}"
  id_tokens:
    DDOCTOSTS_ID_TOKEN:
      aud: dd-octo-sts
  script:
    - |
      echo "UNPIN_DEPENDENCIES: ${UNPIN_DEPENDENCIES:-}"
      echo "NIGHTLY_BUILD: ${NIGHTLY_BUILD:-}"
    - |
      if [ -z ${GH_TOKEN} ]
      then
        # Use dd-octo-sts to get GitHub token
        export GH_TOKEN=$(dd-octo-sts token --scope DataDog/dd-trace-py --policy gitlab.github-access.read)
      fi
    - scripts/gen_gitlab_config.py --verbose
  needs: []
  artifacts:
    paths:
      - .gitlab/tests-gen.yml
      - .gitlab/benchmarks/microbenchmarks-gen.yml
      - .gitlab/benchmarks/bp-runner.microbenchmarks.fail-on-breach.yml

run-tests-trigger:
 stage: tests
 needs: [ tests-gen ]
 variables:
   UNPIN_DEPENDENCIES: "${UNPIN_DEPENDENCIES:-}"
   NIGHTLY_BUILD: "${NIGHTLY_BUILD:-}"
 # Allow the child job to fail if explicitly asked
 rules:
   - if: $RELEASE_ALLOW_TEST_FAILURES == "true"
     allow_failure: true
   - allow_failure: false
 trigger:
   include:
     - artifact: .gitlab/tests-gen.yml
       job: tests-gen
   strategy: depend

serverless lambda tests:
  stage: tests
  trigger:
    project: DataDog/datadog-lambda-python
    strategy: depend
    branch: main
  needs:
    - job: "upload all"
  variables:
    UPSTREAM_PIPELINE_ID: $CI_PIPELINE_ID
    UPSTREAM_PROJECT_URL: $CI_PROJECT_URL
    UPSTREAM_COMMIT_BRANCH: $CI_COMMIT_BRANCH
    UPSTREAM_COMMIT_AUTHOR: $CI_COMMIT_AUTHOR
    UPSTREAM_COMMIT_TITLE: $CI_COMMIT_TITLE
    UPSTREAM_COMMIT_TAG: $CI_COMMIT_TAG
    UPSTREAM_PROJECT_NAME: $CI_PROJECT_NAME
    UPSTREAM_GITLAB_USER_LOGIN: $GITLAB_USER_LOGIN
    UPSTREAM_GITLAB_USER_EMAIL: $GITLAB_USER_EMAIL
    SKIP_E2E_TESTS: "true"

# Validate the ast-grep rule's test suite in .sg/tests
"ast-grep rules":
  extends: .testrunner
  stage: tests
  needs: []
  script:
  script:
    - |
      echo -e "\e[0Ksection_start:`date +%s`:sg_test[collapsed=true]\r\e[0KValidate ast-grep rules"
      hatch run lint:sg-test
      echo -e "\e[0Ksection_end:`date +%s`:sg_test\r\e[0K"
    - |
      echo -e "\e[0Ksection_start:`date +%s`:sg_scan[collapsed=true]\r\e[0Kast-grep scan"
      hatch run lint:sg
      echo -e "\e[0Ksection_end:`date +%s`:sg_scan\r\e[0K"

microbenchmarks:
  stage: benchmarks
  needs:
    - job: tests-gen
    - job: "build linux"
      parallel:
        matrix:
          - ARCH_TAG: "amd64"
            PYTHON_TAG: "cp39-cp39"
            IMAGE_TAG: "v85383392-751efc0-manylinux2014_x86_64"
  rules:
    # Allow failures if explicitly asked
    - if: $RELEASE_ALLOW_BENCHMARK_FAILURES == "true"
      allow_failure: true
    - allow_failure: false
  trigger:
    include:
      - artifact: .gitlab/benchmarks/microbenchmarks-gen.yml
        job: tests-gen
    strategy: depend
  variables:
    PARENT_PIPELINE_ID: $CI_PIPELINE_ID
    # Disable VPA for benchmarks
    DD_DISABLE_VPA: true

macrobenchmarks:
  stage: benchmarks
  needs:
    - job: "build linux"
      parallel:
        matrix:
          - ARCH_TAG: "amd64"
            PYTHON_TAG: "cp39-cp39"
            IMAGE_TAG: "v85383392-751efc0-manylinux2014_x86_64"
  trigger:
    include: .gitlab/benchmarks/macrobenchmarks.yml
  variables:
    PARENT_PIPELINE_ID: $CI_PIPELINE_ID
    # Disable VPA for benchmarks
    DD_DISABLE_VPA: true
  allow_failure: false
  rules:
    # Always run on nightly schedule or API triggered pipelines
    - if: $NIGHTLY_BUILD == "true"
      when: always
    # Allow failures if explicitly asked
    - if: $RELEASE_ALLOW_BENCHMARK_FAILURES == "true"
      allow_failure: true
    # Always run on tagged releases and release branches
    - if: $CI_COMMIT_BRANCH =~ /^[0-9]+\.[0-9]+$/
      when: always
    - if: !reference [.is_release]
      when: always
    # Otherwise, run manually
    - when: manual
      allow_failure: true

check_new_flaky_tests:
  stage: tests
  needs: ["run-tests-trigger"]
  extends: .testrunner
  script:
    - export DD_SITE=datadoghq.com
    - export DD_API_KEY=$(aws ssm get-parameter --region us-east-1 --name ci.${CI_PROJECT_NAME}.dd-api-key-qualitygate --with-decryption --query "Parameter.Value" --out text)
    - export DD_APP_KEY=$(aws ssm get-parameter --region us-east-1 --name ci.${CI_PROJECT_NAME}.dd-app-key-qualitygate --with-decryption --query "Parameter.Value" --out text)
    - datadog-ci gate evaluate
  rules:
    - if: !reference [.is_main]
      when: never
    - if: !reference [.is_release]
      when: never
    - if: !reference [.is_release_branch]
      when: never
    - if: !reference [.is_gh_merge_queue]
      when: never
    - if: !reference [.is_devflow_merge_queue]
      when: never
    - when: on_success

codeql-scan:
  timeout: 30m
  stage: tests
  needs: []
  image:
    name: registry.ddbuild.io/images/code-scanning:v76675231-865a08a-default
  tags: [ "arch:amd64" ]
  id_tokens:
    DDOCTOSTS_ID_TOKEN:
      aud: dd-octo-sts
  rules:
    - if: !reference [.is_release]
      when: never
    - if: $NIGHTLY_BUILD == "true"
      when: never
    - when: on_success
  variables:
    AWS_REGION: us-east-1
    BASE_REF: main
    GOMAXPROCS: 10
    KUBERNETES_CPU_REQUEST: 10
    KUBERNETES_CPU_LIMIT: 10
    KUBERNETES_MEMORY_REQUEST: 96Gi
    KUBERNETES_MEMORY_LIMIT: 96Gi
    CI_PROJECT_NAMESPACE: DataDog
    CODEQL: /usr/local/codeql/codeql
    CODEQL_DB: /tmp/dd-trace-py.codeql
    PYTHON_CUSTOM_QLPACK: /tmp/codescanning/qlpacks/python/codeql-suites/python-code-scanning.qls
    DB_CONFIGS: --threads 8 --ram 96000 --language=python --quiet
    SCAN_CONFIGS: --format sarifv2.1.0 --threads 8 --ram 96000 --no-tuple-counting --quiet
    UPLOAD_CONFIGS: -upload_sarif=true
  script:
    - ./scripts/codeql_scan.sh

requirements_json_test:
  rules:
    - when: on_success
  variables:
    REQUIREMENTS_BLOCK_JSON_PATH: ".gitlab/requirements_block.json"
    REQUIREMENTS_ALLOW_JSON_PATH: ".gitlab/requirements_allow.json"

check_requirements_lockfiles:
  stage: tests
  needs: []
  extends: .testrunner
  script:
    - pip install toml==0.10.2
    - scripts/compile-and-prune-test-requirements
    - scripts/check-diff '.riot/requirements/' 'Mismatches found between .riot/requirements/*.txt and riotfile.py. Run scripts/compile-and-prune-test-requirements and commit the result.'
    - python scripts/requirements_to_csv.py
    - scripts/check-diff 'requirements.csv' 'Tracer dependency requirements in requirements.csv is out of date. Run `python scripts/requirements_to_csv.py` and commit the result.'

check_ci_dependencies:
  stage: tests
  needs: []
  extends: .testrunner
  script:
    - scripts/check-ci-dependencies

codeowners:
  stage: tests
  needs: []
  tags: ["arch:amd64"]
  image: registry.ddbuild.io/images/mirror/ubuntu:25.04
  script:
    # Validate that each file entry only exists once in CODEOWNERS to prevent confusion about ownership.
    # The file is evaluated from top to bottom where the last match wins, so duplicate entries can lead to unexpected owners being assigned.
    # Normalize patterns before checking for duplicates:
    #   - Remove trailing slashes (tests/tracer and tests/tracer/ are equivalent)
    #   - Remove leading ./ (./path and path are equivalent)
    #   - Collapse multiple consecutive slashes (path//file becomes path/file)
    - |
        # All non-comment/empty lines from .github/CODEOWNERS
        awk '!/^#|^$/ {print $1}' .github/CODEOWNERS | \
          # Remove any leading `./` in the line
          sed 's|^\./||' | \
          # Remove any duplicate `/`, e.g. `//` -> `/`
          sed -E 's|/+|/|g' | \
          # Remove any trailing `/`
          sed 's:/*$::' | \
          # Sort and return only duplicate entries
          sort | uniq -d > /tmp/duplicate_codeowners.txt
        if [ -s /tmp/duplicate_codeowners.txt ]; then
            echo "Duplicate entries found in .github/CODEOWNERS for patterns (after normalization):"
            cat /tmp/duplicate_codeowners.txt | sed 's/^/  - /'
            echo ""
            echo "Each file pattern should only be listed once to ensure clear ownership."
            echo "Normalization applied: removed trailing slashes, leading './', and collapsed multiple slashes."
            echo "Please remove duplicates from .github/CODEOWNERS."
            exit 1
        fi
    - apt-get update && apt install ca-certificates git wget curl -y
    - wget https://github.com/hmarr/codeowners/releases/download/v1.2.1/codeowners_1.2.1_linux_amd64.tar.gz
    - tar -xvzf codeowners_1.2.1_linux_amd64.tar.gz
    - git diff-tree --no-commit-id --name-only -r $CI_COMMIT_SHA > /tmp/changed_all.txt
    # Filter to only files that exist on disk (deleted files cause codeowners to crash)
    - |
      > /tmp/changed_files.txt
      while IFS= read -r f; do
        [ -f "$f" ] && echo "$f" >> /tmp/changed_files.txt
      done < /tmp/changed_all.txt
    - |
      if [ -s /tmp/changed_files.txt ]; then
        echo '```' > codeowners.txt
        xargs ./codeowners < /tmp/changed_files.txt | tee -a codeowners.txt
        echo '```' >> codeowners.txt
      fi
    - |
      export MESSAGE="$(awk '{printf "%s\\n", $0}' codeowners.txt | sed 's/\"/\\"/g')"
    - |
      if [ ! -z "$MESSAGE" ]; then
        wget -nv binaries.ddbuild.io/dd-source/authanywhere/LATEST/authanywhere-linux-amd64
        chmod +x authanywhere-linux-amd64
        curl 'https://pr-commenter.us1.ddbuild.io/internal/cit/pr-comment' \
          -H "$(./authanywhere-linux-amd64)" \
          -X PATCH -d "{ \
            \"commit\": \"$CI_COMMIT_SHORT_SHA\", \
            \"message\": \"$MESSAGE\", \
            \"header\": \"Codeowners resolved as\", \
            \"org\": \"Datadog\", \
            \"repo\": \"dd-trace-py\" \
          }"
      fi

detect_circular_imports:
  stage: tests
  needs: []
  extends: .testrunner
  rules:
    - if: !reference [.is_main]
      when: never
  script:
    - cd ..
    - python -m pip install -r dd-trace-py/scripts/import-analysis/requirements-cycles.txt
    - cp dd-trace-py/scripts/import-analysis/cycles.py .
    - python dd-trace-py/scripts/import-analysis/cycles.py analyze cycles-pr.json
    - cat cycles-pr.json
    - cd dd-trace-py && git checkout main && cd ..
    - python dd-trace-py/scripts/import-analysis/cycles.py analyze cycles-base.json
    - cat cycles-base.json
    - python dd-trace-py/scripts/import-analysis/cycles.py compare cycles-base.json cycles-pr.json > cycles_report.txt || true
    - CYCLES_REPORT="$(cat cycles_report.txt)"
    - cat cycles_report.txt
    - |
      if [ ! -z "$CYCLES_REPORT" ]; then
        wget -nv binaries.ddbuild.io/dd-source/authanywhere/LATEST/authanywhere-linux-amd64
        chmod +x authanywhere-linux-amd64
        export MESSAGE="$(awk '{printf "%s\\n", $0}' cycles_report.txt | sed 's/\"/\\"/g')"
        curl 'https://pr-commenter.us1.ddbuild.io/internal/cit/pr-comment' \
          -H "$(./authanywhere-linux-amd64)" \
          -X PATCH -d "{ \
            \"commit\": \"$CI_COMMIT_SHORT_SHA\", \
            \"message\": \"$MESSAGE\", \
            \"header\": \"\", \
            \"org\": \"Datadog\", \
            \"repo\": \"dd-trace-py\" \
          }"
        exit 1
      fi


package-oci:
  needs:
    - job: "package version"
      artifacts: true
    - job: "build linux"
      artifacts: true
    - job: "download_dependency_wheels"
      artifacts: true

promote-oci-to-prod:
  stage: release
  rules:
    - if: !reference [.is_release]
      when: on_success
    - when: never
  needs:
    - job: release_pypi_prod
    - job: package-oci
      artifacts: true
    - job: oci-internal-publish
      artifacts: true

promote-oci-to-prod-beta:
  stage: release
  needs:
    - job: package-oci
      artifacts: true
    - job: oci-internal-publish
      artifacts: true

promote-oci-to-staging:
  stage: release
  needs:
    - job: package-oci
      artifacts: true
    - job: oci-internal-publish
      artifacts: true

publish-lib-init-pinned-tags:
  stage: release
  rules:
    - if: !reference [.is_release]
      when: on_success
    - when: never
  needs:
    - job: release_pypi_prod
    - job: create-multiarch-lib-injection-image
    - job: generate-lib-init-pinned-tag-values
      artifacts: true

configure_system_tests:
  variables:
    SYSTEM_TESTS_SCENARIOS_GROUPS: "simple_onboarding,simple_onboarding_profiling,simple_onboarding_appsec,docker-ssi,lib-injection"

deploy_to_reliability_env:
  needs: []

deploy_to_di_backend:manual:
  stage: shared-pipeline
  rules:
    - when: manual
      allow_failure: true
  trigger:
    project: DataDog/debugger-demos
    branch: main
  variables:
    UPSTREAM_PROJECT_ID: $CI_PROJECT_ID
    UPSTREAM_PROJECT_NAME: $CI_PROJECT_NAME
    UPSTREAM_COMMIT_SHORT_SHA: $CI_COMMIT_SHORT_SHA
    UPSTREAM_PIPELINE_ID: $CI_PIPELINE_ID
    UPSTREAM_COMMIT_AUTHOR: $CI_COMMIT_AUTHOR
    UPSTREAM_TAG: $CI_COMMIT_TAG
    UPSTREAM_PACKAGE_JOB: build

lib_injection_tests:
  # tests for sitecustomize safety that intentionally run outside of riot to simulate a fresh system
  needs: []
  stage: tests
  tags: [ "arch:amd64" ]
  image: registry.ddbuild.io/images/mirror/ubuntu:25.04
  parallel:
    matrix:
      - PYTHON: [ "2.7", "3.5", "3.6", "3.7", "3.8", "3.9", "3.10", "3.11", "3.12", "3.13", "3.14", "3.15-dev" ]
  script:
    - export PYENV_ROOT="${HOME}/.pyenv"
    - export PATH="${PYENV_ROOT}/shims:${PYENV_ROOT}/bin:${PATH}"
    - apt-get update && apt install build-essential zlib1g-dev libssl-dev curl git -y
    - PYENV_GIT_TAG=v2.6.17 curl https://pyenv.run | bash
    - pyenv --version
    - pyenv install ${PYTHON} && pyenv global ${PYTHON}
    - python --version
    - python lib-injection/sources/sitecustomize.py

# Profiling native tests with sanitizers
# Uses dedicated profiling-native image with Python, Rust, valgrind, and clang pre-installed
# Image built from: dd/images/dd-trace-py/profiling_native
# Only runs when profiling-related files change
.profiling_native_base:
  needs: []
  stage: tests
  tags: [ "arch:amd64" ]
  image: ${PROFILING_NATIVE_IMAGE}
  timeout: 90m
  variables:
    CMAKE_BUILD_PARALLEL_LEVEL: "12"
    CARGO_BUILD_JOBS: "12"
  before_script:
    - ulimit -c unlimited
    - git config --global --add safe.directory ${CI_PROJECT_DIR}
    - export PYENV_VERSION=${PYTHON_VERSION}
  script:
    - python --version
    - if [ "${MEMCPY_MODE}" == "fast" ]; then export ECHION_USE_FAST_COPY_MEMORY=1; else export ECHION_USE_FAST_COPY_MEMORY=0; fi
    # All profiling tests use EXPECT_EXIT (death tests) which fork child processes.
    # LSAN's signal handler in forked children conflicts with gtest, causing spurious crashes.
    # Disable leak detection for "safety" - Valgrind jobs already cover leak detection.
    - if [ "${SANITIZER}" == "safety" ]; then export ASAN_OPTIONS=detect_leaks=0; fi
    - ./ddtrace/internal/datadog/profiling/build_standalone.sh --${SANITIZER} RelWithDebInfo stack_test

# Run on PRs where files in the native Profiling build graph changed.
# build_standalone.sh compiles Rust (setup.py build_rust) then builds C++ tests
# via CMake, so Rust, C/C++, Cython, and CMake files should all trigger it.
# Extension-based patterns keep Python and docs out automatically; non-extension
# build files (CMakeLists.txt, .sh, .supp) are listed separately.
profiling_native:
  extends: .profiling_native_base
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_COMMIT_REF_NAME == "main"
    - changes:
        - .gitlab-ci.yml
        # Native source files by extension (C, C++, Cython, CMake modules)
        - ddtrace/internal/datadog/profiling/**/*.cpp
        - ddtrace/internal/datadog/profiling/**/*.cc
        - ddtrace/internal/datadog/profiling/**/*.h
        - ddtrace/internal/datadog/profiling/**/*.hpp
        - ddtrace/internal/datadog/profiling/**/*.pyx
        - ddtrace/internal/datadog/profiling/**/*.cmake
        - ddtrace/profiling/**/*.cpp
        - ddtrace/profiling/**/*.cc
        - ddtrace/profiling/**/*.h
        - ddtrace/profiling/**/*.hpp
        - ddtrace/profiling/**/*.pyx
        # Build/test config files (not matchable by extension alone)
        - ddtrace/internal/datadog/profiling/**/CMakeLists.txt
        - ddtrace/internal/datadog/profiling/**/*.sh
        - ddtrace/internal/datadog/profiling/**/*.supp
        - ddtrace/profiling/**/CMakeLists.txt
        # Rust source (build_standalone.sh compiles Rust first; CMake links
        # against generated FFI headers from src/native/target/.../include/)
        - src/native/**/*.rs
        - src/native/**/*.toml
        - src/native/**/Cargo.lock
        # Top-level build config
        - setup.py
        - pyproject.toml
  parallel:
    matrix:
      - PYTHON_VERSION: ["3.9", "3.10", "3.11", "3.12", "3.13", "3.14"]
        MEMCPY_MODE: ["default"]
        SANITIZER: ["safety", "thread", ""]
      # Valgrind on subset only: ASAN/LSAN (in "safety") already cover leaks, buffer overflows,
      # and use-after-free. Valgrind's main added value is detecting uninitialized memory usage,
      # which is worth checking on a few versions without the slowdown on all runs.
      # If needed, please add more versions to the list.
      - PYTHON_VERSION: ["3.12", "3.14"]
        MEMCPY_MODE: ["default"]
        SANITIZER: ["valgrind"]
      - PYTHON_VERSION: ["3.9", "3.10", "3.11", "3.12", "3.13", "3.14"]
        MEMCPY_MODE: ["fast"]
        SANITIZER: ["safety", "thread"]
