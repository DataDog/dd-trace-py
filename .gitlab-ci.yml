stages:
  - package
  - deploy
  - benchmarks
  - benchmarks-pr-comment
  - macrobenchmarks

include:
  - remote: https://gitlab-templates.ddbuild.io/apm/packaging.yml
  - local: ".gitlab/benchmarks.yml"
  - local: ".gitlab/macrobenchmarks.yml"

variables:
  DOWNSTREAM_BRANCH:
    value: "master"
    description: "Run a specific datadog-reliability-env branch downstream"
  DOWNSTREAM_MBP_BRANCH:
    value: "dd-trace-py"
    description: "Run a specific relenv-microbenchmarking-platform branch downstream"
  PYTHON_PACKAGE_VERSION:
    description: "Version to build for .deb and .rpm. Must be already published in PyPi"

.common: &common
  tags: [ "runner:main", "size:large" ]

package:
  extends: .package
  rules:
  - if: $PYTHON_PACKAGE_VERSION
    when: on_success
  - if: '$CI_COMMIT_TAG =~ /^v.*/'
    when: on_success
  script:
    - ../.gitlab/build-deb-rpm.sh
    - find . -iregex '.*\.\(deb\|rpm\)' -printf '%f\0' | xargs -0 dd-pkg lint

package-arm:
  extends: .package-arm
  rules:
  - if: $PYTHON_PACKAGE_VERSION
    when: on_success
  - if: '$CI_COMMIT_TAG =~ /^v.*/'
    when: on_success
  script:
    - ../.gitlab/build-deb-rpm.sh
    - find . -iregex '.*\.\(deb\|rpm\)' -printf '%f\0' | xargs -0 dd-pkg lint

.release-package:
  stage: deploy
  variables:
    PRODUCT_NAME: auto_inject-python

deploy_to_reliability_env:
  stage: deploy
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: on_success
    - when: manual
      allow_failure: true
  trigger:
    project: DataDog/apm-reliability/datadog-reliability-env
    branch: $DOWNSTREAM_BRANCH
  variables:
    UPSTREAM_PROJECT_ID: $CI_PROJECT_ID
    UPSTREAM_PROJECT_NAME: $CI_PROJECT_NAME
    UPSTREAM_BRANCH: $CI_COMMIT_REF_NAME
    UPSTREAM_COMMIT_SHA: $CI_COMMIT_SHA

deploy_to_di_backend:manual:
  stage: deploy
  rules:
    - when: manual
      allow_failure: true
  trigger:
    project: DataDog/debugger-demos
    branch: main
  variables:
    UPSTREAM_PROJECT_ID: $CI_PROJECT_ID
    UPSTREAM_PROJECT_NAME: $CI_PROJECT_NAME
    UPSTREAM_COMMIT_SHORT_SHA: $CI_COMMIT_SHORT_SHA
    UPSTREAM_PIPELINE_ID: $CI_PIPELINE_ID
    UPSTREAM_COMMIT_AUTHOR: $CI_COMMIT_AUTHOR
    UPSTREAM_TAG: $CI_COMMIT_TAG
    UPSTREAM_PACKAGE_JOB: build

deploy_to_docker_registries:
  stage: deploy
  rules:
    - if: '$POPULATE_CACHE'
      when: never
    - if: '$CI_COMMIT_TAG =~ /^v.*/'
      when: on_success
    - when: manual
      allow_failure: true
  trigger:
    project: DataDog/public-images
    branch: main
    strategy: depend
  variables:
    IMG_SOURCES: ghcr.io/datadog/dd-trace-py/dd-lib-python-init:$CI_COMMIT_TAG
    IMG_DESTINATIONS: dd-lib-python-init:$CI_COMMIT_TAG
    IMG_SIGNING: "false"
    # Wait 4 hours to trigger the downstream job.
    # This is a work-around since there isn't a way to trigger
    # Gitlab from the Github workflow (build_deploy.yml:upload_pypi).
    #
    # The caveat here is that if there is a failure to build to PyPI and it
    # isn't fixed in the retry period then this job will fail and images will
    # not be published.
    RETRY_DELAY: 14400
    RETRY_COUNT: 3

deploy_latest_tag_to_docker_registries:
  stage: deploy
  rules:
    - if: '$POPULATE_CACHE'
      when: never
    - if: '$CI_COMMIT_TAG =~ /^v.*/'
      when: on_success
    - when: manual
      allow_failure: true
  trigger:
    project: DataDog/public-images
    branch: main
    strategy: depend
  variables:
    IMG_SOURCES: ghcr.io/datadog/dd-trace-py/dd-lib-python-init:$CI_COMMIT_TAG
    IMG_DESTINATIONS: dd-lib-python-init:latest
    IMG_SIGNING: "false"
    # See above note in the `deploy_to_docker_registries` job.
    RETRY_DELAY: 14400
    RETRY_COUNT: 3
