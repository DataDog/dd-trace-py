---
fixes:
  - |
    LLM Observability: This fix resolves an issue where the OpenAI, Anthropic, and AWS Bedrock integrations were always setting ``temperature`` and ``max_tokens`` 
    parameters to LLM invocations. The OpenAI integration in particular was setting the wrong ``temperature`` default values. 
    These parameters are now only set if provided in the request.
