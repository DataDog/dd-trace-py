---
fixes:
  - |
    LLM Observability: This fix resolves an issue where bedrock prompt caching caused input token counts to be the 
      number of non-cached tokens instead of the total tokens sent to the model.
