---
features:
  - |
    LLM Observability: When not using a provider integration (OpenAI, Anthropic, or Bedrock) with the LangChain integration, token metrics will be appended to the LLM Observability ``llm`` span.
    LLM Observability: When ``llm.with_structured_output(..., method="json_mode")``, or ``response_format={"type": "json_object"}`` is passed into the llm invocation, the LLM Observability will be an ``llm`` span instead of a ``workflow`` span.                 
