---
fixes:
  - |
    LLM Observability: This fix resolves an issue where anthropic prompt caching caused input token metric to be the 
      number of non-cached tokens instead of the total tokens sent to the model.
