---
features:
  - |
    LLM Observability: This introduces the ability to track the number of tokens read from the cache for OpenAI prompt caching.
