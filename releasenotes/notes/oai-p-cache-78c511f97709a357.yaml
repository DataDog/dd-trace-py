---
features:
  - |
    LLM Observability: This introduces the ability to track the number of tokens read from the prompt cache for OpenAI Chat Completions.
