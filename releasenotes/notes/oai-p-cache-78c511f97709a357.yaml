---
features:
  - |
    LLM Observability: Introduces tracking the count of cached input tokens for OpenAI Chat Completions prompt caching.
