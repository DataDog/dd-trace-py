# Conditional Wrapping Optimization Analysis

## Optimization Idea
**Only wrap locks when profiling is active** - avoid the overhead of wrapping locks when the profiler is not running or when lock profiling is disabled.

## Current Architecture

### How Lock Wrapping Works Now

1. **Profiler Lifecycle:**
   - Profiler starts ‚Üí `LockCollector._start_service()` ‚Üí `patch()` called
   - `patch()` replaces `threading.Lock` with `_profiled_allocate_lock` wrapper function
   - **ALL** lock creations go through wrapper while profiler is running
   - Profiler stops ‚Üí `unpatch()` ‚Üí restores original `threading.Lock`

2. **Lock Creation:**
   ```python
   # After patch(), every threading.Lock() call creates:
   _ProfiledLock(
       wrapped=original_lock(*args, **kwargs),
       tracer=self.tracer,
       max_nframes=self.nframes,
       capture_sampler=self._capture_sampler,
   )
   ```

3. **Lock Operations:**
   ```python
   def _acquire(self, inner_func, *args, **kwargs):
       if not self.capture_sampler.capture():  # Fast path
           return inner_func(*args, **kwargs)
       
       # Slow path: profiling enabled
       start = time.monotonic_ns()
       try:
           return inner_func(*args, **kwargs)
       finally:
           end = time.monotonic_ns()
           self.acquired_time = end
           self._update_name()
           self._flush_sample(start, end, is_acquire=True)
   ```

## Overhead Analysis

### Current Overhead When NOT Sampling (`capture_pct < 100`)

**Per Lock Creation:**
- ‚úÖ **Minimal**: Just creates `_ProfiledLock` wrapper object
- Frame inspection to get init location (sys._getframe(3), once)
- Small memory overhead per lock (~200 bytes)

**Per Lock Acquire/Release:**
- ‚úÖ **Very Low**: Just 1 function call overhead + capture check
- `capture_sampler.capture()`: increment counter + comparison (~2-3 CPU cycles)
- If not capturing: immediately delegates to wrapped lock
- **No** `time.monotonic_ns()` calls
- **No** frame inspection
- **No** `_update_name()` call

**Evidence from tests/profiling/collector/test_threading.py:1130-1158:**
```python
def test_lock_profiling_overhead_reasonable(self):
    # Test allows up to 50x overhead for 0% capture
    # This is generous because lock operations are microsecond-level
```
The 50x allowance suggests the overhead is measurable but not catastrophic.

### Current Overhead When Profiler is STOPPED

**After `profiler.stop()`:**
- ‚úÖ **Zero overhead**: `unpatch()` restores original lock
- New locks created after stop ‚Üí no wrapper at all
- **BUT**: Locks created BEFORE stop remain wrapped forever
  - They keep checking `capture_sampler.capture()` 
  - But capture rate is effectively 0 after stop (collector stopped)

## Feasibility Analysis

### Option 1: Don't Wrap Until Profiler Starts ‚úÖ **ALREADY IMPLEMENTED**

**Status:** ‚úÖ This already happens!
- Lock wrapping only occurs when `LockCollector.start()` is called
- If profiler is never started, zero overhead
- If profiler is stopped, new locks have zero overhead

**Savings:** None (already optimal for this case)

### Option 2: Unwrap Locks When Profiler Stops ‚ùå **NOT FEASIBLE**

**Goal:** Remove wrapper from locks when profiling stops to eliminate overhead

**Major Challenges:**

1. **No Global Registry:**
   - No tracking of created `_ProfiledLock` instances
   - Locks can be stored anywhere: globals, instance vars, closures, local vars
   - Cannot find and modify all references

2. **Identity Preservation:**
   - Locks used as dictionary keys (identity-based hashing)
   - Unwrapping would break `lock is lock` identity checks
   - Would break any code holding references to the lock object

3. **Thread Safety:**
   - Locks might be held during unwrapping
   - Race conditions between unwrap and acquire/release
   - Undefined behavior if lock state changes during operation

4. **Implementation Complexity:**
   - Would need global weak reference registry (memory overhead)
   - Would need synchronization for registry access (lock overhead!)
   - Would need careful handling of lock state during transition

**Verdict:** ‚ùå Not feasible without major architectural changes

### Option 3: Fast-Path Bypass When Collector Stopped ‚ö†Ô∏è **MARGINAL BENEFIT**

**Idea:** Check `collector.status == RUNNING` in `_acquire`/`_release` before capture check

**Implementation:**
```python
def _acquire(self, inner_func, *args, **kwargs):
    # Add this check:
    if not self.collector_is_running():
        return inner_func(*args, **kwargs)
    
    if not self.capture_sampler.capture():
        return inner_func(*args, **kwargs)
    # ... rest unchanged
```

**Challenges:**

1. **Need Collector Reference:**
   - Would need `_ProfiledLock` to hold reference to collector
   - Extra memory per lock (~8 bytes for pointer)
   - Circular reference concerns (lock ‚Üí collector ‚Üí lock)

2. **Thread-Safe Status Check:**
   - Collector status is protected by `_service_lock`
   - Would need lock acquisition for every status check
   - **This adds lock overhead to avoid profiling overhead!** (ironic)

3. **Minimal Savings:**
   - Only saves the `capture_sampler.capture()` call
   - That's just: `self._counter += self.capture_pct; if self._counter >= 100: ...`
   - ~2-3 CPU cycles vs checking collector status (~10-20 cycles with lock)
   - **Net negative performance impact!**

**Verdict:** ‚ö†Ô∏è Not worth the complexity and likely makes performance worse

### Option 4: Conditional Wrapping Based on Lock "Hotness" ü§î **INTERESTING**

**Idea:** Only wrap locks that are actually contended/frequently used

**Implementation:**
- Initial locks are **unwrapped** (zero overhead)
- Track lock contention at module/function level
- After detecting contention, **dynamically wrap** hot locks
- Use bytecode manipulation or `sys.settrace` to detect lock patterns

**Challenges:**

1. **Detection Overhead:**
   - Need monitoring to identify hot locks (overhead!)
   - When to start/stop monitoring?

2. **Wrapping Existing Locks:**
   - Same problems as Option 2 (identity preservation, references)

3. **Complexity:**
   - Very complex heuristics needed
   - Risk of missing important locks
   - Risk of wrapping too many locks anyway

**Verdict:** ü§î Theoretically interesting but practically very complex for uncertain gain

## Measured Performance (Real Numbers)

### Benchmark Results
Running 1,000,000 lock acquire/release cycles on macOS (Apple Silicon):

| Scenario | Time per op | Overhead | vs Baseline |
|----------|-------------|----------|-------------|
| **Baseline** (no profiling) | 56.25 ns | - | 1.0x |
| **0% capture** | 643.38 ns | +587 ns | **11.4x** ‚ö†Ô∏è |
| **1% capture** (DEFAULT) | 713.16 ns | +657 ns | **12.7x** ‚ö†Ô∏è |
| **100% capture** | 6211.21 ns | +6155 ns | **110.4x** üî• |
| **After stop** (lock created before) | 707.27 ns | +651 ns | **12.6x** ‚ö†Ô∏è |
| **After stop** (lock created after) | 54.34 ns | -2 ns | **1.0x** ‚úÖ |

### Key Findings

**‚ö†Ô∏è WARNING: Overhead is Higher Than Initially Estimated!**

1. **Wrapper overhead is ~10-12x baseline**, not the 2-5x I initially estimated
2. This translates to **~650 nanoseconds per lock operation** when not capturing
3. For a high-throughput application:
   - 1M lock ops/sec ‚Üí **650ms CPU overhead/sec** (65% of 1 core!)
   - 10M lock ops/sec ‚Üí **6.5 seconds CPU overhead/sec** (6.5 cores!)

### Why Is The Overhead So High?

Looking at the implementation, even when NOT capturing (`capture_pct=0`), each lock operation:

1. **Function call overhead:** `_acquire()` wrapper function call
2. **Attribute lookups:** `self.capture_sampler.capture()`
3. **Method calls:** `capture()` (even though it returns False)
4. **Counter manipulation:** `self._counter += self.capture_pct; if self._counter >= 100: ...`
5. **Python function call overhead:** Delegation to `inner_func(*args, **kwargs)`

In Python, function call overhead is significant. Even a simple wrapper adds ~500-600ns overhead.

## Revised Savings Analysis

### Scenario 1: Profiler Never Started
- **Current:** Zero overhead ‚úÖ
- **After optimization:** Zero overhead
- **Savings:** None (already optimal)

### Scenario 2: Profiler Running, Low Capture Rate (1%)
- **Current overhead:** ~650-700 ns per operation ‚ö†Ô∏è
- **After optimization:** Cannot improve further without unwrapping
- **Impact:** 
  - For lock-light apps (<10K ops/sec): ~6ms CPU/sec (negligible)
  - For lock-heavy apps (>1M ops/sec): ~650ms CPU/sec (significant!)

### Scenario 3: Profiler Stopped, Locks Still Wrapped
- **Current overhead:** ~650 ns per operation ‚ö†Ô∏è
- **Locks created after stop:** Zero overhead ‚úÖ
- **Locks created before stop:** Keep wrapper overhead (~650ns)

**Potential savings IF we could unwrap pre-existing locks:**
- Eliminate ~650ns per operation
- For 1M lock ops/sec: saves **650ms CPU/sec** (0.65 cores)
- For 10M lock ops/sec: saves **6.5 seconds CPU/sec** (6.5 cores)
- **BUT:** Still not feasible per Option 2 analysis

### Scenario 4: Lock Profiling Disabled (DD_PROFILING_LOCK_ENABLED=false)
- **Current:** LockCollector never created, zero overhead
- **After optimization:** Same
- **Savings:** None (already optimal)

## Recommendations

### ‚ö†Ô∏è REVISED ASSESSMENT: Overhead is Higher Than Expected

The measured overhead of **~650ns per lock operation** (10-12x baseline) is **significant** for lock-heavy applications.

**Impact Assessment:**
- **Low lock usage** (<10K ops/sec): Negligible (~6ms CPU/sec)
- **Medium lock usage** (100K ops/sec): Noticeable (~65ms CPU/sec)  
- **High lock usage** (1M ops/sec): **Significant** (~650ms CPU/sec = 0.65 cores)
- **Very high lock usage** (10M ops/sec): **Severe** (~6.5 sec CPU/sec = 6.5 cores)

### ‚ùå STILL DO NOT PURSUE Conditional Unwrapping

**Despite the higher-than-expected overhead**, unwrapping remains **not feasible** due to:
- No way to find all lock references
- Identity preservation requirements  
- Thread safety concerns
- High implementation complexity
- Risk of breaking user code

### ‚ö†Ô∏è DO NOT ADD Collector Status Checks

Adding collector status checks would likely **decrease performance**:
- Adds synchronization overhead (acquiring lock to check status)
- Would only save the `capture()` call (~50-100ns)
- Still leaves ~500-550ns of function call overhead
- Net negative benefit

### ‚úÖ FEASIBLE ALTERNATIVES TO REDUCE OVERHEAD

Given the measured overhead, here are **actionable optimizations** that avoid the unwrapping problem:

#### 1. **Optimize the Fast Path** (HIGH IMPACT) üéØ

**Current bottleneck:** Even at 0% capture, we have ~650ns overhead from Python function calls.

**Optimization:** Move hot path to C/Cython:
```python
# Current (Python):
def _acquire(self, inner_func, *args, **kwargs):
    if not self.capture_sampler.capture():  # ~50ns
        return inner_func(*args, **kwargs)  # ~500ns delegation
    # ... profiling logic

# Optimized (Cython):
cdef class _ProfiledLock:
    cdef object __wrapped__
    cdef CaptureSampler capture_sampler
    
    cdef inline acquire(self, *args, **kwargs):
        # Inlined capture check + direct delegation
        # Could reduce overhead from 650ns ‚Üí ~50-100ns
```

**Estimated savings:** ~500-600ns per operation (85-90% reduction in overhead!)  
**Complexity:** Medium (requires Cython rewrite of hot path)  
**Risk:** Low (isolated change, well-tested domain)

#### 2. **Inline the Wrapper** (MEDIUM IMPACT)

**Idea:** Instead of wrapping at object creation, monkey-patch `_thread.lock` methods directly

```python
# Current: Create wrapper object for each lock
_ProfiledLock(wrapped=original_lock())

# Alternative: Patch lock methods directly
import _thread
_original_acquire = _thread.lock.acquire

def _profiled_acquire(self, *args, **kwargs):
    if not _should_profile():
        return _original_acquire(self, *args, **kwargs)
    # ... profiling logic

_thread.lock.acquire = _profiled_acquire
```

**Benefits:**
- Eliminates wrapper object overhead
- Reduces memory per lock
- Simpler to disable (just unpatch methods)

**Challenges:**
- Harder to track per-lock state (acquired_time, name)
- Would need thread-local or global tracking dict
- More invasive monkey-patching

**Estimated savings:** ~200-300ns per operation  
**Complexity:** High (significant refactoring)  
**Risk:** Medium (thread safety concerns)

#### 3. **Better Configuration & Documentation** (LOW EFFORT, IMMEDIATE)

**Quick wins:**

```python
# Add environment variable to disable lock profiling entirely
DD_PROFILING_LOCK_ENABLED=false  # Already exists! Just need better docs

# Add per-module filtering
DD_PROFILING_LOCK_EXCLUDE_MODULES=django,sqlalchemy,requests
# Don't wrap locks from these modules (common lock-heavy frameworks)

# Add adaptive disabling
DD_PROFILING_LOCK_AUTO_DISABLE_THRESHOLD_OPS_SEC=1000000
# Automatically disable if lock ops/sec exceeds threshold
```

**Estimated savings:** Depends on user configuration  
**Complexity:** Very low (documentation + config options)  
**Risk:** None

#### 4. **Lazy Profiling Activation** (MEDIUM IMPACT)

**Idea:** Start with profiling disabled, enable only when needed

```python
# Start profiler without lock collector
profiler = Profiler(_lock_collector_enabled=False)
profiler.start()

# Enable lock profiling on-demand (e.g., via remote config)
profiler.enable_lock_profiling()

# Or: automatically enable when contention detected
if detect_high_lock_contention():
    profiler.enable_lock_profiling()
```

**Benefits:**
- Zero overhead until explicitly enabled
- Can enable/disable dynamically
- Works well for ad-hoc investigation

**Challenges:**
- Won't profile locks created before enabling
- Requires detection mechanism

**Estimated savings:** 100% for applications that don't need lock profiling  
**Complexity:** Low (use existing start/stop mechanism)  
**Risk:** Low

#### 5. **Smarter Sampling Strategy** (MEDIUM IMPACT)

**Current:** Global `capture_pct` applies to all locks equally

**Optimized:** Per-lock adaptive sampling
```python
class _AdaptiveSampler:
    def __init__(self):
        self.lock_stats = {}  # lock_id -> (contentions, samples)
    
    def should_capture(self, lock_id):
        stats = self.lock_stats[lock_id]
        if stats.contentions > THRESHOLD:
            return True  # Always sample contended locks
        else:
            return sample_at_low_rate()  # Rare sampling for uncontended
```

**Benefits:**
- Focus profiling on actually contended locks
- Reduce overhead for uncontended locks
- Better data quality (more samples from hot locks)

**Estimated savings:** Depends on contention patterns  
**Complexity:** Medium  
**Risk:** Low

## Conclusion

### Main Proposal: Conditional Unwrapping

**Feasibility: ‚ùå Not feasible** for conditional unwrapping when profiler stops  
**Potential Savings: ~650ns per op** (~650ms CPU/sec per 1M ops/sec)  
**Actual Savings: ~0%** (cannot implement due to technical constraints)

**Why NOT feasible:**
- No way to track and unwrap all lock references
- Would break object identity (`lock is lock`)
- Thread safety impossibly complex
- High risk of breaking user code

### Revised Assessment Based on Measurements

The measured overhead is **higher than initially estimated**:
- ‚ùå **Wrong estimate:** "5-10ns overhead" ‚Üí **Actually: 650ns overhead** (130x worse!)
- ‚ö†Ô∏è **Impact:** For lock-heavy apps (1M ops/sec), this is **0.65 CPU cores of overhead**

**However**, conditional unwrapping is still not the solution because:
1. **Technically infeasible** (see above)
2. **Only helps after profiler stops** (rare scenario)
3. **Better alternatives exist** (see below)

### ‚úÖ RECOMMENDED ACTION PLAN

**Short Term (Immediate):**
1. ‚úÖ **Better documentation** about lock profiling overhead
2. ‚úÖ **Recommend `DD_PROFILING_LOCK_ENABLED=false`** for lock-heavy apps
3. ‚úÖ **Document impact**: ~650ns/op, ~0.65 cores per 1M ops/sec

**Medium Term (Next Quarter):**
1. üéØ **Optimize fast path with Cython** (estimated 85-90% overhead reduction)
2. üéØ **Add per-module filtering** (`DD_PROFILING_LOCK_EXCLUDE_MODULES`)
3. üéØ **Add adaptive auto-disable** for high lock operation rates

**Long Term (Future):**
1. ü§î **Investigate C-level instrumentation** instead of Python wrappers
2. ü§î **Per-lock adaptive sampling** (focus on contended locks)
3. ü§î **eBPF-based lock profiling** (Linux only, zero overhead when not sampling)

### Impact on Production

**Current state:**
- Lock profiling is **enabled by default** (`DD_PROFILING_LOCK_ENABLED=true`)
- Default `capture_pct=1.0` (1% sampling)
- **All customers** pay ~650ns overhead per lock operation

**Risk Assessment:**
| Lock Ops/Sec | Overhead | Impact | Recommendation |
|--------------|----------|--------|----------------|
| < 10K | ~6ms CPU/sec | ‚úÖ Negligible | Keep enabled |
| 10K - 100K | 6-60ms CPU/sec | ‚ö†Ô∏è Noticeable | Monitor |
| 100K - 1M | 60-650ms CPU/sec | ‚ö†Ô∏è Significant | Consider disabling |
| > 1M | > 650ms CPU/sec | üî• Severe | **Disable immediately** |

### Bottom Line

**Conditional unwrapping: ‚ùå Don't pursue** (not feasible)

**Instead:**
1. ‚úÖ **Document the overhead** (650ns/op) 
2. ‚úÖ **Provide better configuration options** (per-module filtering, auto-disable)
3. üéØ **Optimize the fast path** (Cython rewrite ‚Üí 85-90% overhead reduction)

The measured overhead is significant enough to warrant optimization, but **not via conditional unwrapping**. Focus on reducing overhead during active profiling instead.

