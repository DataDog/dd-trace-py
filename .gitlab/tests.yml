stages:
  - setup
  - riot
  - hatch
  - exploration

variables:
  RIOT_RUN_CMD: riot -P -v run --exitfirst --pass-env -s
  REPO_LANG: python # "python" is used everywhere rather than "py"
  # CI_DEBUG_SERVICES: "true"

include:
  - local: ".gitlab/services.yml"
  - local: ".gitlab/testrunner.yml"

.test_base_hatch:
  extends: .testrunner
  stage: hatch
  # Hatch doesn't use pre-built wheels or venvs so we can start them right away
  needs: [ prechecks ]
  parallel: 4
  # DEV: This is the max retries that GitLab currently allows for
  before_script:
    - !reference [.testrunner, before_script]
  script:
    - export PYTEST_ADDOPTS="${PYTEST_ADDOPTS} --ddtrace"
    - export _DD_CIVISIBILITY_USE_CI_CONTEXT_PROVIDER=true
    - export DD_FAST_BUILD="1"
    - |
      envs=( $(hatch env show --json | jq -r --arg suite_name "$SUITE_NAME" 'keys[] | select(. | contains($suite_name))' | sort | ./.gitlab/ci-split-input.sh) )
      if [[ ${#envs[@]} -eq 0 ]]; then
        echo "No hatch envs found for ${SUITE_NAME}"
        exit 1
      fi
      for env in "${envs[@]}"
      do
        echo "Running hatch env: ${env}:test"
        hatch run ${env}:test
      done
  variables:
    CMAKE_BUILD_PARALLEL_LEVEL = "12"
    CARGO_BUILD_JOBS = "12"
    DD_FAST_BUILD = "1"
    PIP_EXTRA_INDEX_URL ="https://download.pytorch.org/whl/cu124"



.test_base_hatch_snapshot:
  extends: .test_base_hatch
  services:
    - !reference [.services, testagent]
  before_script:
    - !reference [.test_base_hatch, before_script]
    # DEV: All job variables get shared with services, setting `DD_TRACE_AGENT_URL` on the testagent will tell it to forward all requests to the
    # agent at that host. Therefore setting this as a variable will cause recursive requests to the testagent
    - export DD_TRACE_AGENT_URL="http://testagent:9126"

# GPU variants of base templates
.test_base_hatch_gpu:
  extends: .testrunner_gpu
  stage: hatch
  needs: [ prechecks ]
  parallel: 2
  variables:
    # Request==Limit to achieve Guaranteed QoS; requires Kubernetes executor support
    KUBERNETES_MEMORY_REQUEST: "10Gi"
    KUBERNETES_MEMORY_LIMIT: "10Gi"
    KUBERNETES_CPU_REQUEST: "2"
    KUBERNETES_CPU_LIMIT: "2"
  before_script:
    - !reference [.testrunner_gpu, before_script]
    # GPU diagnostics
    - nvidia-smi || true
    - free -h || true
    - export DD_VLLM_TEST_DIAG="1"
    - export MALLOC_ARENA_MAX="2"
    - export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:64,expandable_segments:True"
    - export VLLM_DISABLE_FLASH_ATTENTION="1"
    - export VLLM_LOGGING_LEVEL="DEBUG"
    - export OMP_NUM_THREADS="1"
    - export MKL_NUM_THREADS="1"
    - export NUMEXPR_NUM_THREADS="1"
    - |
      python - <<'PY'
      import sys
      try:
          import torch
          print("torch:", torch.__version__, "cuda:", getattr(torch.version, "cuda", None), "is_available:", torch.cuda.is_available())
      except Exception as e:
          print("torch import error:", e, file=sys.stderr)
      try:
          import vllm
          print("vllm:", getattr(vllm, "__version__", "unknown"))
      except Exception as e:
          print("vllm import error:", e, file=sys.stderr)
      PY
    - ls -l /usr/lib/x86_64-linux-gnu/libcuda* || true
  script:
    - export PYTEST_ADDOPTS="${PYTEST_ADDOPTS} --ddtrace"
    - export _DD_CIVISIBILITY_USE_CI_CONTEXT_PROVIDER=true
    - export DD_FAST_BUILD="1"
    - |
      envs=( $(hatch env show --json | jq -r --arg suite_name "$SUITE_NAME" 'keys[] | select(. | contains($suite_name))' | sort | ./.gitlab/ci-split-input.sh) )
      if [[ ${#envs[@]} -eq 0 ]]; then
        echo "No hatch envs found for ${SUITE_NAME}"
        exit 1
      fi
      for env in "${envs[@]}"
      do
        echo "Running hatch env (GPU): ${env}:test"
        hatch run ${env}:test
      done
  variables:
    CMAKE_BUILD_PARALLEL_LEVEL = "12"
    CARGO_BUILD_JOBS = "12"
    DD_FAST_BUILD = "1"
    PIP_EXTRA_INDEX_URL = "https://download.pytorch.org/whl/cu124"

.test_base_hatch_gpu_snapshot:
  extends: .test_base_hatch_gpu
  services:
    - !reference [.services, testagent]
  before_script:
    - !reference [.test_base_hatch_gpu, before_script]
    - export DD_TRACE_AGENT_URL="http://testagent:9126"

# Do not define a `needs:` in order to depend on the whole `precheck` stage
.test_base_riot:
  extends: .testrunner
  stage: riot
  needs: [ build_base_venvs, prechecks ]
  parallel: 4
  services:
    - !reference [.services, ddagent]
  # DEV: This is the max retries that GitLab currently allows for
  before_script:
    - !reference [.testrunner, before_script]
    - unset DD_SERVICE
    - unset DD_ENV
    - unset DD_TAGS
    - unset DD_TRACE_REMOVE_INTEGRATION_SERVICE_NAMES_ENABLED
    - export PYTEST_ADDOPTS="${PYTEST_ADDOPTS} -vv -s -rA"
  script:
    - |
      hashes=( $(.gitlab/scripts/get-riot-hashes.sh "${SUITE_NAME}") )
      if [[ ${#hashes[@]} -eq 0 ]]; then
        echo "No riot hashes found for ${SUITE_NAME}"
        exit 1
      fi
      for hash in "${hashes[@]}"
      do
        echo "Running riot hash: ${hash}"
        riot list "${hash}"
        export _CI_DD_TAGS="test.configuration.riot_hash:${hash}"
        ${RIOT_RUN_CMD} "${hash}" -- --ddtrace
      done
      ./scripts/check-diff ".riot/requirements/" \
        "Changes detected after running riot. Consider deleting changed files, running scripts/compile-and-prune-test-requirements and committing the result."
      ./scripts/check-diff "ddtrace/contrib/integration_registry/registry.yaml" \
        "Registry YAML file (ddtrace/contrib/integration_registry/registry.yaml) was modified. Please run: scripts/integration_registry/update_and_format_registry.py and commit the changes."


.test_base_riot_snapshot:
  extends: .test_base_riot
  services:
    - !reference [.test_base_riot, services]
    - !reference [.services, testagent]
  before_script:
    - !reference [.test_base_riot, before_script]
    # DEV: All job variables get shared with services, setting `DD_TRACE_AGENT_URL` on the testagent will tell it to forward all requests to the
    # agent at that host. Therefore setting this as a variable will cause recursive requests to the testagent
    - export DD_TRACE_AGENT_URL="http://testagent:9126"
    - ln -s "${CI_PROJECT_DIR}" "/home/bits/project"

.test_base_riot_gpu:
  extends: .testrunner_gpu
  stage: riot
  needs: [ build_base_venvs, prechecks ]
  parallel: 2
  variables:
    KUBERNETES_MEMORY_REQUEST: "10Gi"
    KUBERNETES_MEMORY_LIMIT: "10Gi"
    KUBERNETES_CPU_REQUEST: "2"
    KUBERNETES_CPU_LIMIT: "2"
  services:
    - !reference [.services, ddagent]
  before_script:
    - !reference [.testrunner_gpu, before_script]
    # GPU diagnostics
    - nvidia-smi || true
    - free -h || true
    - |
      python - <<'PY'
      import sys
      try:
          import torch
          print("torch:", torch.__version__, "cuda:", getattr(torch.version, "cuda", None), "is_available:", torch.cuda.is_available())
      except Exception as e:
          print("torch import error:", e, file=sys.stderr)
      try:
          import vllm
          print("vllm:", getattr(vllm, "__version__", "unknown"))
      except Exception as e:
          print("vllm import error:", e, file=sys.stderr)
      PY
    - ls -l /usr/lib/x86_64-linux-gnu/libcuda* || true
    - unset DD_SERVICE
    - unset DD_ENV
    - unset DD_TAGS
    - unset DD_TRACE_REMOVE_INTEGRATION_SERVICE_NAMES_ENABLED
    - export PYTEST_ADDOPTS="${PYTEST_ADDOPTS} -vv -s -rA"
    - export DD_VLLM_TEST_DIAG="1"
    - export MALLOC_ARENA_MAX="2"
    - export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:64,expandable_segments:True"
    - export VLLM_DISABLE_FLASH_ATTENTION="1"
    - export VLLM_LOGGING_LEVEL="DEBUG"
    - export OMP_NUM_THREADS="1"
    - export MKL_NUM_THREADS="1"
    - export NUMEXPR_NUM_THREADS="1"
    - export VLLM_GPU_MEMORY_UTILIZATION="${VLLM_GPU_MEMORY_UTILIZATION:-0.1}"
    - export TOKENIZERS_PARALLELISM="false"
    - export VLLM_HOST_IP="127.0.0.1"
  script:
    - |
      hashes=( $(.gitlab/scripts/get-riot-hashes.sh "${SUITE_NAME}") )
      if [[ ${#hashes[@]} -eq 0 ]]; then
        echo "No riot hashes found for ${SUITE_NAME}"
        exit 1
      fi
      for hash in "${hashes[@]}"
      do
        echo "Running riot hash (GPU): ${hash}"
        riot list "${hash}"
        export _CI_DD_TAGS="test.configuration.riot_hash:${hash}"
        ${RIOT_RUN_CMD} "${hash}" -- --ddtrace
      done
      ./scripts/check-diff ".riot/requirements/" \
        "Changes detected after running riot. Consider deleting changed files, running scripts/compile-and-prune-test-requirements and committing the result."
      ./scripts/check-diff "ddtrace/contrib/integration_registry/registry.yaml" \
        "Registry YAML file (ddtrace/contrib/integration_registry/registry.yaml) was modified. Please run: scripts/integration_registry/update_and_format_registry.py and commit the changes."

.test_base_riot_gpu_snapshot:
  extends: .test_base_riot_gpu
  services:
    - !reference [.test_base_riot_gpu, services]
    - !reference [.services, testagent]
  before_script:
    - !reference [.test_base_riot_gpu, before_script]
    - export DD_TRACE_AGENT_URL="http://testagent:9126"
    - ln -s "${CI_PROJECT_DIR}" "/home/bits/project"

# Required jobs will appear here
