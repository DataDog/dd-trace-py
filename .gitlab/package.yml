compute_library_version:
  image: registry.ddbuild.io/images/dd-octo-sts-ci-base:2025.06-1
  tags: [ "arch:amd64" ]
  stage: package
  id_tokens:
    DDOCTOSTS_ID_TOKEN:
      aud: dd-octo-sts
  script: |
    if [ -z ${GH_TOKEN} ]
    then
      # Use dd-octo-sts to get GitHub token
      dd-octo-sts token --scope DataDog/dd-trace-py --policy gitlab.github-access.read > token
      gh auth login --with-token < token
      rm token
    fi
    # Prevent git operation errors:
    #   failed to determine base repo: failed to run git: fatal: detected dubious ownership in repository at ...
    git config --global --add safe.directory "${CI_PROJECT_DIR}"
    .gitlab/download-library-version-from-gh-actions.sh

    echo "SETUPTOOLS_SCM_PRETEND_VERSION_FOR_DDTRACE=$(cat library-version/version.txt)" | tee library_version.env
    echo "DDTRACE_VERSION=$(cat library-version/version.txt)" | tee -a library_version.env
  artifacts:
    reports:
      dotenv: library_version.env
    paths:
      - "library-version/version.txt"

download_ddtrace_artifacts:
  image: registry.ddbuild.io/images/dd-octo-sts-ci-base:2025.06-1
  tags: [ "arch:amd64" ]
  stage: package
  id_tokens:
    DDOCTOSTS_ID_TOKEN:
      aud: dd-octo-sts
  script: |
    if [ -z ${GH_TOKEN} ]
    then
      # Use dd-octo-sts to get GitHub token
      dd-octo-sts token --scope DataDog/dd-trace-py --policy gitlab.github-access.read > token
      gh auth login --with-token < token
      rm token
    fi
    # Prevent git operation errors:
    #   failed to determine base repo: failed to run git: fatal: detected dubious ownership in repository at ...
    git config --global --add safe.directory "${CI_PROJECT_DIR}"
    .gitlab/download-wheels-from-gh-actions.sh
  artifacts:
    paths:
      - "pywheels/*.whl"
      - "pywheels/*.tar.gz"

download_dependency_wheels:
  image: registry.ddbuild.io/images/mirror/python:$PYTHON_IMAGE_TAG
  tags: [ "arch:amd64" ]
  stage: package
  needs: [ download_ddtrace_artifacts ]
  variables:
    PIP_CACHE_DIR: "${CI_PROJECT_DIR}/.cache/pip"
  parallel:
    matrix: # The image tags that are mirrored are in: https://github.com/DataDog/images/blob/master/mirror.yaml
      - PYTHON_IMAGE_TAG: "3.8"
        PYTHON_VERSION: "3.8"
      - PYTHON_IMAGE_TAG: "3.9.13"
        PYTHON_VERSION: "3.9"
      - PYTHON_IMAGE_TAG: "3.10.13"
        PYTHON_VERSION: "3.10"
      - PYTHON_IMAGE_TAG: "3.11.6"
        PYTHON_VERSION: "3.11"
      - PYTHON_IMAGE_TAG: "3.12.0"
        PYTHON_VERSION: "3.12"
      - PYTHON_IMAGE_TAG: "3.13.0"
        PYTHON_VERSION: "3.13"
      - PYTHON_IMAGE_TAG: "3.14.0"
        PYTHON_VERSION: "3.14"
  script:
    - .gitlab/download-dependency-wheels.sh
  cache:
    - key: v0-download-dep-wheels-cache
      paths:
        - ${CI_PROJECT_DIR}/.cache/
  artifacts:
    paths:
      - "pywheels-dep/"

publish-wheels-to-s3:
  tags: ["arch:amd64"]
  image: registry.ddbuild.io/images/mirror/amazon/aws-cli:2.4.29
  stage: package
  needs:
    - job: download_ddtrace_artifacts
      artifacts: true
    - job: compute_library_version
      artifacts: true
  variables:
    BUCKET: dd-trace-py-builds
  script:
    - set -euo pipefail
    - shopt -s nullglob
    # Find wheels
    - WHEELS=(pywheels/*.whl)
    - |
      if [ ${#WHEELS[@]} -eq 0 ]; then
        echo "No wheels found in pywheels/"; exit 1
      fi
    - echo "Found ${#WHEELS[@]} wheel(s):"
    - printf ' - %s\n' "${WHEELS[@]}"

    - |
      if [ -f library-version/version.txt ]; then
        VERSION="$(tr -d '\r\n' < library-version/version.txt)"
      fi

      if [ -z "${VERSION:-}" ]; then
        echo "ERROR: VERSION is not defined or library-version/version.txt missing!"
        exit 1
      fi

    - printf 'Detected version %s\n' ${VERSION}

    # Upload all wheels to versioned prefix and pipeline-id prefix
    - aws s3 cp --recursive --exclude "*" --include "*.whl" pywheels "s3://${BUCKET}/${VERSION}/"
    - aws s3 cp --recursive --exclude "*" --include "*.whl" pywheels "s3://${BUCKET}/${CI_PIPELINE_ID}/"

    - |
      VERSION_ENC="${VERSION//+/%2B}"
      S3_BASE_VER="https://${BUCKET}.s3.amazonaws.com/${VERSION_ENC}"
      S3_BASE_PIPE="https://${BUCKET}.s3.amazonaws.com/${CI_PIPELINE_ID}"

      generate_index_html() {
        local outfile="$1"
        {
          echo "<html><body>"
          for w in "${WHEELS[@]}"; do
            fname="$(basename "$w")"
            enc_fname="${fname//+/%2B}"
            echo "<a href=\"${enc_fname}\">${fname}</a><br>"
          done
          echo "</body></html>"
        } > "${outfile}"
      }

      # Generate both minimal indexes
      generate_index_html "index.version.html"
      generate_index_html "index.pipeline.html"

      # Upload to each S3 prefix
      aws s3 cp "index.version.html" "s3://${BUCKET}/${VERSION}/index.html" --content-type text/html
      aws s3 cp "index.pipeline.html" "s3://${BUCKET}/${CI_PIPELINE_ID}/index.html" --content-type text/html

      # Print the clickable URLs
      VER_INDEX_URL="${S3_BASE_VER}/index.html"
      PIPE_INDEX_URL="${S3_BASE_PIPE}/index.html"
      echo "S3 index (version):  ${VER_INDEX_URL}"
      echo "S3 index (pipeline): ${PIPE_INDEX_URL}"
