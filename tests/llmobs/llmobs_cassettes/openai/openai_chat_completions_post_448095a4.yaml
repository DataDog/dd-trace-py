interactions:
- request:
    body: '{"messages": [{"content": "You are a {role} assistant.", "role": "system"},
      {"content": "Your expertise is in software engineering.", "role": "system"},
      {"content": "Additional context: focus on best practices", "role": "system"},
      {"content": "I''m a user seeking help.", "role": "user"}, {"content": "Please
      help with code review", "role": "user"}, {"content": "Specifically, I need performance
      optimization", "role": "user"}, {"content": "I understand your request.", "role":
      "assistant"}, {"content": "I''ll help you with code review.", "role": "assistant"},
      {"content": "Let me provide recommendations", "role": "assistant"}, {"content":
      "Make it thorough and under 200 words", "role": "developer"}], "model": "gpt-3.5-turbo",
      "stream": false}'
    headers:
      ? !!python/object/apply:multidict._multidict.istr
      - Accept
      : - application/json
      ? !!python/object/apply:multidict._multidict.istr
      - Accept-Encoding
      : - gzip, deflate, zstd
      ? !!python/object/apply:multidict._multidict.istr
      - Connection
      : - keep-alive
      Content-Length:
      - '745'
      ? !!python/object/apply:multidict._multidict.istr
      - Content-Type
      : - application/json
      ? !!python/object/apply:multidict._multidict.istr
      - User-Agent
      : - AsyncOpenAI/Python 1.61.1
      ? !!python/object/apply:multidict._multidict.istr
      - X-Stainless-Arch
      : - arm64
      ? !!python/object/apply:multidict._multidict.istr
      - X-Stainless-Async
      : - async:asyncio
      ? !!python/object/apply:multidict._multidict.istr
      - X-Stainless-Lang
      : - python
      ? !!python/object/apply:multidict._multidict.istr
      - X-Stainless-OS
      : - MacOS
      ? !!python/object/apply:multidict._multidict.istr
      - X-Stainless-Package-Version
      : - 1.61.1
      ? !!python/object/apply:multidict._multidict.istr
      - X-Stainless-Runtime
      : - CPython
      ? !!python/object/apply:multidict._multidict.istr
      - X-Stainless-Runtime-Version
      : - 3.10.13
      ? !!python/object/apply:multidict._multidict.istr
      - x-stainless-retry-count
      : - '0'
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: "{\n  \"id\": \"chatcmpl-CIesnYp3ArbeZmlvm2lTqpctcCYrZ\",\n  \"object\":
        \"chat.completion\",\n  \"created\": 1758563093,\n  \"model\": \"gpt-3.5-turbo-0125\",\n
        \ \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\":
        \"assistant\",\n        \"content\": \"When optimizing for performance, focus
        on reducing unnecessary computations or I/O operations, minimizing memory
        usage, and improving algorithm efficiency. Consider using data structures
        like hash maps for quick lookups and minimizing nested loops to reduce time
        complexity. Profile your code using tools like profilers to identify bottlenecks.
        Utilize caching mechanisms where applicable to reduce redundant computations.
        Use asynchronous processing for I/O-bound operations to improve responsiveness.
        Remember to balance readability with performance optimizations and document
        rationale behind any non-intuitive optimizations for future reference. Always
        test performance improvements with realistic data and scenarios to validate
        the effectiveness of the optimizations.\",\n        \"refusal\": null,\n        \"annotations\":
        []\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n
        \   }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 109,\n    \"completion_tokens\":
        118,\n    \"total_tokens\": 227,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\":
        0,\n      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\":
        {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"accepted_prediction_tokens\":
        0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"service_tier\":
        \"default\",\n  \"system_fingerprint\": null\n}\n"
    headers:
      CF-RAY:
      - 98339ce46f8b4ce8-BOS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Mon, 22 Sep 2025 17:44:54 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=IK_Vjf0AvDicpP_jyKYa1Fnxkvw5GvGBXQQHcxK5k04-1758563094-1.0.1.1-9Qi8HJOL2Z6zHU58xhszX5yAunjAEVs4Q7Ler05IN2B2IgS5QNEUhnOgdizPWHTI_kXpyYYCgC9YbSQny4Ryi6j9PPCZz9NorlBcNNFDeIs;
        path=/; expires=Mon, 22-Sep-25 18:14:54 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=LtuICazb21rxIhiZxZ72FV8xzAwsyzGaETrRwMBnW2o-1758563094420-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Strict-Transport-Security:
      - max-age=31536000; includeSubDomains; preload
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      access-control-expose-headers:
      - X-Request-ID
      alt-svc:
      - h3=":443"; ma=86400
      cf-cache-status:
      - DYNAMIC
      openai-organization:
      - datadog-staging
      openai-processing-ms:
      - '1064'
      openai-project:
      - proj_gt6TQZPRbZfoY2J9AQlEJMpd
      openai-version:
      - '2020-10-01'
      x-envoy-upstream-service-time:
      - '1098'
      x-openai-proxy-wasm:
      - v0.1
      x-ratelimit-limit-requests:
      - '10000'
      x-ratelimit-limit-tokens:
      - '50000000'
      x-ratelimit-remaining-requests:
      - '9999'
      x-ratelimit-remaining-tokens:
      - '49999905'
      x-ratelimit-reset-requests:
      - 6ms
      x-ratelimit-reset-tokens:
      - 0s
      x-request-id:
      - req_7fde04bd038e40aa989b221d8f96ffb8
    status:
      code: 200
      message: OK
version: 1
